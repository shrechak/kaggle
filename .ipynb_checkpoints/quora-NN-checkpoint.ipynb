{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download('punkt')\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words(question) :\n",
    "    return list(filter(lambda word: word not in stop and word.isalpha(), nltk.word_tokenize(question.lower())))\n",
    "\n",
    "getLabeledSentencetrain = lambda given_tuple: [\n",
    "             LabeledSentence(get_words(given_tuple[0]),[\"trainquestion1_\"+str(given_tuple[2])]),  \n",
    "             LabeledSentence(get_words(given_tuple[1]),[\"trainquestion2_\"+str(given_tuple[2])])]\n",
    "\n",
    "getLabeledSentencetest = lambda given_tuple: [\n",
    "             LabeledSentence(get_words(given_tuple[0]),[\"testquestion1_\"+str(given_tuple[2])]),  \n",
    "             LabeledSentence(get_words(given_tuple[1]),[\"testquestion2_\"+str(given_tuple[2])])]\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf = pd.read_csv('train.csv')\n",
    "traindf['index1'] = traindf.index\n",
    "traindf = traindf.fillna(\"\")\n",
    "traindf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf = pd.read_csv('train.csv')\n",
    "testdf = testdf.fillna(\"\")\n",
    "testdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindf = traindf[:1000]\n",
    "testdf = testdf[:1000]\n",
    "\n",
    "train_df_size = traindf.shape[0]\n",
    "test_df_size = testdf.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainSentences = flatten(list(map(lambda x: getLabeledSentencetrain(x) , traindf[['question1', 'question2', 'id']].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testSentences = flatten(list(map(lambda x: getLabeledSentencetest(x) , testdf[['question1', 'question2', 'id']].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = trainSentences + testSentences\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=1000, sample=1e-4, negative=5, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 12:16:31,148 : INFO : collecting all words and their counts\n",
      "2017-04-26 12:16:31,152 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-04-26 12:16:31,193 : INFO : collected 3594 word types and 4000 unique tags from a corpus of 4000 examples and 20460 words\n",
      "2017-04-26 12:16:31,195 : INFO : Loading a fresh vocabulary\n",
      "2017-04-26 12:16:31,214 : INFO : min_count=1 retains 3594 unique words (100% of original 3594, drops 0)\n",
      "2017-04-26 12:16:31,215 : INFO : min_count=1 leaves 20460 word corpus (100% of original 20460, drops 0)\n",
      "2017-04-26 12:16:31,246 : INFO : deleting the raw counts dictionary of 3594 items\n",
      "2017-04-26 12:16:31,247 : INFO : sample=0.0001 downsamples 946 most-common words\n",
      "2017-04-26 12:16:31,252 : INFO : downsampling leaves estimated 13709 word corpus (67.0% of prior 20460)\n",
      "2017-04-26 12:16:31,253 : INFO : estimated required memory for 3594 words and 1000 dimensions: 47349000 bytes\n",
      "2017-04-26 12:16:31,273 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 12:16:33,078 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-26 12:16:33,080 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-26 12:16:34,113 : INFO : PROGRESS: at 39.20% examples, 34402 words/s, in_qsize 7, out_qsize 1\n",
      "2017-04-26 12:16:34,115 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-26 12:16:34,123 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-26 12:16:34,127 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-26 12:16:34,133 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-26 12:16:34,138 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-26 12:16:34,228 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-26 12:16:34,449 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-26 12:16:34,452 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-26 12:16:34,453 : INFO : training on 102300 raw words (88533 effective words) took 1.3s, 65843 effective words/s\n",
      "2017-04-26 12:16:34,454 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-26 12:16:34,460 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-26 12:16:34,464 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-26 12:16:35,571 : INFO : PROGRESS: at 9.81% examples, 7935 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-26 12:16:35,649 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-26 12:16:35,671 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-26 12:16:35,677 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-26 12:16:35,679 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-26 12:16:35,690 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-26 12:16:35,766 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-26 12:16:35,967 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-26 12:16:35,984 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-26 12:16:35,985 : INFO : training on 102300 raw words (88477 effective words) took 1.5s, 58709 effective words/s\n",
      "2017-04-26 12:16:35,986 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-26 12:16:35,991 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-26 12:16:35,992 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-26 12:16:36,987 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-26 12:16:36,999 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-26 12:16:37,008 : INFO : PROGRESS: at 58.62% examples, 51462 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-26 12:16:37,009 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-26 12:16:37,017 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-26 12:16:37,055 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-26 12:16:37,115 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-26 12:16:37,337 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-26 12:16:37,346 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-26 12:16:37,347 : INFO : training on 102300 raw words (88670 effective words) took 1.3s, 65809 effective words/s\n",
      "2017-04-26 12:16:37,352 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-26 12:16:37,358 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-26 12:16:37,359 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-26 12:16:38,492 : INFO : PROGRESS: at 9.65% examples, 7582 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-26 12:16:38,541 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-26 12:16:38,553 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-26 12:16:38,560 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-26 12:16:38,591 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-26 12:16:38,622 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-26 12:16:38,685 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-26 12:16:38,901 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-26 12:16:38,907 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-26 12:16:38,908 : INFO : training on 102300 raw words (88467 effective words) took 1.5s, 57403 effective words/s\n",
      "2017-04-26 12:16:38,909 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-26 12:16:38,914 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-26 12:16:38,919 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-26 12:16:39,949 : INFO : PROGRESS: at 19.59% examples, 17203 words/s, in_qsize 9, out_qsize 0\n",
      "2017-04-26 12:16:39,970 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-26 12:16:39,988 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-26 12:16:40,021 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-26 12:16:40,040 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-26 12:16:40,051 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-26 12:16:40,123 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-26 12:16:40,379 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-26 12:16:40,389 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-26 12:16:40,390 : INFO : training on 102300 raw words (88627 effective words) took 1.4s, 61173 effective words/s\n",
      "2017-04-26 12:16:40,391 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-26 12:16:40,396 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-26 12:16:40,397 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-26 12:16:41,403 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-26 12:16:41,406 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-26 12:16:41,409 : INFO : PROGRESS: at 58.63% examples, 51837 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-26 12:16:41,411 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-26 12:16:41,424 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-26 12:16:41,433 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-26 12:16:41,520 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-26 12:16:41,738 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-26 12:16:41,758 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-26 12:16:41,759 : INFO : training on 102300 raw words (88498 effective words) took 1.4s, 65532 effective words/s\n",
      "2017-04-26 12:16:41,760 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-26 12:16:41,765 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-26 12:16:41,766 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-26 12:16:42,739 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-26 12:16:42,742 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-26 12:16:42,755 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-26 12:16:42,764 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-26 12:16:42,783 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-26 12:16:42,862 : INFO : PROGRESS: at 80.45% examples, 65425 words/s, in_qsize 2, out_qsize 1\n",
      "2017-04-26 12:16:42,864 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-26 12:16:43,090 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-26 12:16:43,092 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-26 12:16:43,093 : INFO : training on 102300 raw words (88554 effective words) took 1.3s, 67114 effective words/s\n",
      "2017-04-26 12:16:43,094 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-26 12:16:43,099 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-26 12:16:43,100 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-26 12:16:44,177 : INFO : PROGRESS: at 9.60% examples, 8034 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-26 12:16:44,212 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-26 12:16:44,233 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-26 12:16:44,265 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-26 12:16:44,289 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-26 12:16:44,310 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-26 12:16:44,387 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-26 12:16:44,657 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-26 12:16:44,659 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-26 12:16:44,660 : INFO : training on 102300 raw words (88654 effective words) took 1.6s, 57153 effective words/s\n",
      "2017-04-26 12:16:44,661 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-26 12:16:44,666 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-26 12:16:44,667 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-26 12:16:45,975 : INFO : PROGRESS: at 9.93% examples, 6666 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-26 12:16:46,004 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-26 12:16:46,006 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-26 12:16:46,012 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-26 12:16:46,032 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-26 12:16:46,045 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-26 12:16:46,127 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-26 12:16:46,382 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-26 12:16:46,386 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-26 12:16:46,386 : INFO : training on 102300 raw words (88723 effective words) took 1.7s, 51871 effective words/s\n",
      "2017-04-26 12:16:46,388 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-26 12:16:46,392 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-26 12:16:46,393 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-26 12:16:47,498 : INFO : PROGRESS: at 9.84% examples, 7963 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-26 12:16:47,575 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-26 12:16:47,592 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-26 12:16:47,598 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-26 12:16:47,602 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-26 12:16:47,606 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-26 12:16:47,676 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-26 12:16:47,905 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-26 12:16:47,907 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-26 12:16:47,908 : INFO : training on 102300 raw words (88538 effective words) took 1.5s, 59063 effective words/s\n",
      "2017-04-26 12:16:47,909 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    random.shuffle(sentences)\n",
    "    model.train(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993717137600685"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.similarity(\"trainquestion1_0\", \"trainquestion2_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "yaxis = []\n",
    "for i in range(0, train_df_size):\n",
    "    sim = model.docvecs.similarity(\"trainquestion1_\"+str(i), \"trainquestion2_\"+str(i))\n",
    "    train_data.append(sim)\n",
    "    train_labels.append(traindf.iloc[i].is_duplicate)\n",
    "    yaxis.append(i)\n",
    "    \n",
    "print(len(train_data))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f528c62e978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(yaxis, train_data, \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = np.array(train_data[:500]).reshape(-1,1)\n",
    "test_labels = np.array(train_labels[:500]).reshape(-1,1)\n",
    "train_data = np.array(train_data).reshape(-1,1)\n",
    "train_labels = np.array(train_labels).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfNN = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/indix/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:904: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfNN.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.622"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfNN.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testId = []\n",
    "test_similarity = []\n",
    "for i in range(0, test_df_size):\n",
    "    sim = model.docvecs.similarity(\"testquestion1_\"+str(i),\"testquestion2_\"+str(i))\n",
    "    testId.append(i)\n",
    "    test_similarity.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99906479849062324,\n",
       " 0.99955643527371596,\n",
       " 0.99971059972532783,\n",
       " 0.9998402471774781,\n",
       " 0.99974274741965075,\n",
       " 0.999793730571235,\n",
       " 0.99966868273915166,\n",
       " 0.99937082258727461,\n",
       " 0.99880258522657095,\n",
       " 0.99923092085373544,\n",
       " 0.99986842415014443,\n",
       " 0.99807165151452482,\n",
       " 0.99892215824329755,\n",
       " 0.99812302575708844,\n",
       " 0.99971236279117792,\n",
       " 0.99765216240920807,\n",
       " 0.99954480678893587,\n",
       " 0.99953683520687786,\n",
       " 0.99953092464246496,\n",
       " 0.99958736288413363,\n",
       " 0.99962527279004476,\n",
       " 0.99964335672930615,\n",
       " 0.99268570877998252,\n",
       " 0.9998505929791599,\n",
       " 0.99960876663085962,\n",
       " 0.99928066632464541,\n",
       " 0.99912915161245741,\n",
       " 0.99978243939480027,\n",
       " 0.68148130775193028,\n",
       " 0.9992537346091056,\n",
       " 0.99384018216750036,\n",
       " 0.99939569045587073,\n",
       " 0.99966411919805043,\n",
       " 0.99957933258012532,\n",
       " 0.99878076706690122,\n",
       " 0.99950218581496852,\n",
       " 0.89497972982330065,\n",
       " 0.9997152285498615,\n",
       " 0.99877698690240924,\n",
       " 0.99947153464045413,\n",
       " 0.99979464574592125,\n",
       " 0.99977033363353085,\n",
       " 0.99919346043911295,\n",
       " 0.99974398681535837,\n",
       " 0.99948300806066626,\n",
       " 0.9975169949868592,\n",
       " 0.99982062920319037,\n",
       " 0.99967051025742137,\n",
       " 0.99958464989286699,\n",
       " 0.99404284804744147,\n",
       " 0.99958205898991392,\n",
       " 0.99978756550553338,\n",
       " 0.99981893188682325,\n",
       " 0.99957013713374754,\n",
       " 0.99969572829393127,\n",
       " 0.99971361462410369,\n",
       " 0.99948035579998173,\n",
       " 0.9995501626430523,\n",
       " 0.99775313305104862,\n",
       " 0.99955741676025534,\n",
       " 0.99981719704846939,\n",
       " 0.99969328583196637,\n",
       " 0.99943288079693804,\n",
       " 0.99937755808960382,\n",
       " 0.99906453451004773,\n",
       " 0.99728427148102572,\n",
       " -0.97334528279118804,\n",
       " 0.99944464017488066,\n",
       " 0.99966603091607098,\n",
       " 0.99975882308800623,\n",
       " 0.99963778831928718,\n",
       " 0.99979824270349993,\n",
       " 0.99926877635404199,\n",
       " 0.99963931775182779,\n",
       " 0.99963999718710395,\n",
       " 0.99968736778208478,\n",
       " 0.99982428793408007,\n",
       " 0.99934006914984552,\n",
       " 0.94717031672658847,\n",
       " 0.9963521000657044,\n",
       " 0.99985387888887578,\n",
       " 0.99914709438465832,\n",
       " 0.99845162010379018,\n",
       " 0.9997297082900507,\n",
       " 0.99929855820005375,\n",
       " 0.99890789010698067,\n",
       " 0.99977145119050082,\n",
       " 0.99977008759543462,\n",
       " 0.99918278792092352,\n",
       " 0.9996451233389948,\n",
       " 0.99962422676789653,\n",
       " 0.99975497990099005,\n",
       " 0.9986190792070555,\n",
       " 0.99960708443121937,\n",
       " 0.99980778124546932,\n",
       " 0.99897790500719319,\n",
       " 0.99987055711548134,\n",
       " 0.99960655526999487,\n",
       " 0.99936949676409426,\n",
       " 0.99156241811105217,\n",
       " 0.99943714074855072,\n",
       " 0.99980733572726421,\n",
       " 0.99965206160217013,\n",
       " 0.99967791576522014,\n",
       " 0.99953346685740718,\n",
       " -0.96207246481345321,\n",
       " 0.99953234994283957,\n",
       " 0.99945874150845271,\n",
       " 0.99953146319503139,\n",
       " 0.9998861985818096,\n",
       " 0.99976719004787584,\n",
       " 0.99714912600373795,\n",
       " 0.99984287421975537,\n",
       " 0.99587563609808927,\n",
       " 0.99909707913323764,\n",
       " 0.9994561339013488,\n",
       " 0.99961710645777713,\n",
       " 0.99973470196013881,\n",
       " 0.99859155383188969,\n",
       " 0.99983999412870994,\n",
       " 0.99381132197536159,\n",
       " 0.99950938815634016,\n",
       " 0.99974945219064204,\n",
       " 0.99972198345099583,\n",
       " 0.99938614587839281,\n",
       " 0.99969704423555228,\n",
       " 0.99802107674428664,\n",
       " 0.99971695298494179,\n",
       " 0.99901418428723676,\n",
       " 0.99642832719978824,\n",
       " 0.99984844049667809,\n",
       " 0.99949403380706592,\n",
       " 0.99963239759973521,\n",
       " 0.99974032969400728,\n",
       " 0.99985078822539364,\n",
       " 0.99895546659475687,\n",
       " 0.99973496804627149,\n",
       " 0.9997807678655809,\n",
       " 0.99919312789104575,\n",
       " 0.999507811263876,\n",
       " 0.99959246145654934,\n",
       " 0.9995645498866208,\n",
       " 0.99979061111905765,\n",
       " 0.99943333313049731,\n",
       " 0.9997085016032774,\n",
       " 0.99952248920029962,\n",
       " 0.99946474362699367,\n",
       " 0.99964630744045224,\n",
       " 0.99975247289572466,\n",
       " 0.99969804557168129,\n",
       " 0.99964062941057397,\n",
       " 0.99955111436345145,\n",
       " 0.99973649602808823,\n",
       " 0.99954408919030424,\n",
       " 0.99948711267800872,\n",
       " 0.99977653995448001,\n",
       " 0.99980706102319827,\n",
       " 0.99972917076993084,\n",
       " 0.99914208119333714,\n",
       " 0.99952040148171273,\n",
       " 0.99933981506833991,\n",
       " 0.99979543046121999,\n",
       " 0.99975365163314867,\n",
       " 0.99957766995948005,\n",
       " 0.99972226364299277,\n",
       " 0.99946675325448631,\n",
       " 0.99984164271585685,\n",
       " 0.99893979858762993,\n",
       " 0.97369430850151084,\n",
       " 0.99978473748157415,\n",
       " 0.99975826979833349,\n",
       " 0.99956614241664732,\n",
       " 0.99930098801156741,\n",
       " 0.99948374328865552,\n",
       " 0.99953879501362564,\n",
       " 0.99974874145054715,\n",
       " 0.99944011713847536,\n",
       " 0.99982303018658902,\n",
       " 0.99939839667885266,\n",
       " 0.9997450202030681,\n",
       " 0.99969393347170066,\n",
       " 0.99876383156371684,\n",
       " 0.99974825091244712,\n",
       " 0.99978853362422415,\n",
       " 0.99952547192385033,\n",
       " 0.99973382431729108,\n",
       " 0.99980292018874051,\n",
       " 0.99960071640448733,\n",
       " 0.93994721565461437,\n",
       " 0.99985123047365854,\n",
       " 0.99740298084089574,\n",
       " 0.99931666814402575,\n",
       " 0.99974988192225311,\n",
       " 0.99960206420839171,\n",
       " 0.99973383742351962,\n",
       " 0.99926200439791957,\n",
       " 0.99950555263183771,\n",
       " 0.99847732478257722,\n",
       " 0.9990866369110013,\n",
       " 0.99956572534725341,\n",
       " 0.99741852125005104,\n",
       " 0.99985762710941961,\n",
       " 0.99716763177319923,\n",
       " 0.99904078746694247,\n",
       " 0.99988387465153716,\n",
       " 0.99978163974362544,\n",
       " 0.99969920580970817,\n",
       " 0.99870590053719754,\n",
       " 0.99984854351509855,\n",
       " 0.99953459123126909,\n",
       " 0.99981103312155817,\n",
       " 0.99954676094213646,\n",
       " 0.99984402473296163,\n",
       " 0.99970096755312376,\n",
       " 0.99981604537172752,\n",
       " 0.99929238699420253,\n",
       " 0.99854702432953957,\n",
       " 0.99970919043725059,\n",
       " 0.99974628760074025,\n",
       " 0.99824184411521166,\n",
       " 0.99034356710296745,\n",
       " 0.99954763592517037,\n",
       " 0.99923666364155206,\n",
       " 0.99967561585515763,\n",
       " 0.9998319239359893,\n",
       " 0.99914898115333117,\n",
       " 0.99915317453500074,\n",
       " 0.99983970007306655,\n",
       " 0.99982174545695879,\n",
       " 0.98061474960102202,\n",
       " 0.99975254867162289,\n",
       " 0.99931241196362119,\n",
       " 0.99976528949702892,\n",
       " 0.9995112282500842,\n",
       " 0.99934396903592571,\n",
       " 0.99832697417734095,\n",
       " 0.99977976639254107,\n",
       " 0.99971070314621302,\n",
       " 0.99951459817471811,\n",
       " 0.99981538067847509,\n",
       " 0.9997356527492226,\n",
       " 0.99974108735187295,\n",
       " 0.99983392161582574,\n",
       " 0.99978725173692251,\n",
       " 0.9987151143361479,\n",
       " 0.99786086936288576,\n",
       " 0.99977206998795853,\n",
       " 0.99960086229985023,\n",
       " 0.99825697126831925,\n",
       " -0.39226059142312947,\n",
       " 0.98275590770563592,\n",
       " 0.99983108907686702,\n",
       " 0.99886671084057232,\n",
       " 0.99973602443831178,\n",
       " 0.99966757407892559,\n",
       " 0.99860450658843214,\n",
       " 0.99965896020822942,\n",
       " 0.97916660674243128,\n",
       " 0.99971428928271822,\n",
       " 0.99974081342364085,\n",
       " 0.98922479350545045,\n",
       " 0.99973666027089347,\n",
       " 0.99954927683085726,\n",
       " 0.99733414892903305,\n",
       " 0.99937194724839729,\n",
       " 0.99963606416469664,\n",
       " 0.99986140236966703,\n",
       " 0.99805560834540807,\n",
       " 0.99856363779551527,\n",
       " 0.99961338183428583,\n",
       " 0.99975324585839498,\n",
       " 0.99963380818842817,\n",
       " 0.99985475687048553,\n",
       " 0.99807534572424084,\n",
       " 0.99939339822579532,\n",
       " 0.99968428933517217,\n",
       " 0.99967438134795117,\n",
       " 0.99893363414354797,\n",
       " 0.99954541623783411,\n",
       " 0.99976539310260371,\n",
       " 0.99985478345647494,\n",
       " 0.99979986942573706,\n",
       " 0.99962666644440312,\n",
       " 0.99961479120611241,\n",
       " -0.97753705163294047,\n",
       " 0.99959528967469968,\n",
       " 0.99963032439930799,\n",
       " 0.93294195661522772,\n",
       " 0.99973189112049221,\n",
       " 0.99967955675148579,\n",
       " 0.99972396004347008,\n",
       " 0.99892720659757017,\n",
       " 0.011427398641784086,\n",
       " 0.99936564568005481,\n",
       " 0.99968017200373671,\n",
       " 0.99957036041867875,\n",
       " 0.99978160492216661,\n",
       " 0.99984166834006005,\n",
       " 0.99983536422195518,\n",
       " 0.99463786892981343,\n",
       " 0.99949608928571809,\n",
       " 0.99979123039326234,\n",
       " 0.9952730426477856,\n",
       " 0.99712453763116171,\n",
       " 0.99965626821965303,\n",
       " 0.99889858477354299,\n",
       " 0.99984964453688907,\n",
       " 0.99954345573040693,\n",
       " 0.99970589922407238,\n",
       " 0.99927254160303081,\n",
       " 0.99967485853632243,\n",
       " 0.99970338777860313,\n",
       " 0.99911566039296473,\n",
       " 0.99983823882593659,\n",
       " 0.99963988519281832,\n",
       " 0.99981128853376533,\n",
       " 0.99959568687264921,\n",
       " 0.99919536699014799,\n",
       " 0.99818417215552602,\n",
       " 0.9969953356344291,\n",
       " 0.99969022606455449,\n",
       " 0.99961793283511191,\n",
       " 0.99955363263452202,\n",
       " 0.99969407627685292,\n",
       " 0.99857883599709396,\n",
       " 0.99937035569914634,\n",
       " 0.99965163539235824,\n",
       " 0.99940674667077656,\n",
       " 0.99940327580777111,\n",
       " 0.99937646906109523,\n",
       " 0.99979441719806295,\n",
       " 0.99976300978497312,\n",
       " 0.99978984403267268,\n",
       " 0.99976824133659559,\n",
       " 0.99983629661046369,\n",
       " 0.9997843768454916,\n",
       " 0.9992855246891188,\n",
       " 0.99977850558934955,\n",
       " 0.99934797149570997,\n",
       " 0.99939364988366997,\n",
       " 0.99717555541839609,\n",
       " 0.99977306700452806,\n",
       " 0.99982904658683769,\n",
       " 0.99901484604993163,\n",
       " 0.999613462022925,\n",
       " 0.99970204036241961,\n",
       " 0.99332112306277087,\n",
       " 0.99729949460302159,\n",
       " 0.99366301838207782,\n",
       " 0.99921712732290346,\n",
       " 0.99906310514979735,\n",
       " 0.99938645145780569,\n",
       " 0.9991693729239427,\n",
       " 0.99723864515870919,\n",
       " 0.9767123750950446,\n",
       " 0.9997291249409056,\n",
       " 0.99921965760300913,\n",
       " 0.99971663305509284,\n",
       " 0.99963680336080352,\n",
       " 0.9994677973305951,\n",
       " 0.99934960555842023,\n",
       " 0.99962998175620521,\n",
       " 0.99931507158277311,\n",
       " 0.99934655331434219,\n",
       " 0.99938352024311417,\n",
       " 0.99941485020739562,\n",
       " 0.99960302166648896,\n",
       " 0.9998431191237519,\n",
       " 0.99976314428366142,\n",
       " 0.99951948997737317,\n",
       " 0.99933334440176913,\n",
       " 0.99673157429850279,\n",
       " 0.99976000148066169,\n",
       " 0.99975393187708739,\n",
       " 0.99986117432899657,\n",
       " 0.99777505906404573,\n",
       " 0.95620750995932002,\n",
       " 0.99975501183453963,\n",
       " 0.99936736867668607,\n",
       " 0.99983786902462923,\n",
       " 0.99958560934102969,\n",
       " 0.99966821912762205,\n",
       " 0.99618541964787999,\n",
       " 0.99857744333502141,\n",
       " 0.99962052524662803,\n",
       " 0.99948738513750068,\n",
       " 0.99983475721128257,\n",
       " 0.99979538528577216,\n",
       " 0.99926107577170131,\n",
       " 0.99984052451484451,\n",
       " 0.99966633885029432,\n",
       " 0.99981490829372888,\n",
       " 0.99965972134858905,\n",
       " 0.99905381411561145,\n",
       " 0.99559196863837829,\n",
       " 0.99957109574691894,\n",
       " 0.99977652689900331,\n",
       " 0.99944579394723232,\n",
       " 0.99978491723891916,\n",
       " 0.99976477959992693,\n",
       " 0.99594970537395477,\n",
       " 0.99820673419288097,\n",
       " 0.99977186826476161,\n",
       " 0.99956527707978171,\n",
       " 0.99981694412745192,\n",
       " 0.9995707722301278,\n",
       " 0.99855988218152314,\n",
       " 0.99984971595488681,\n",
       " 0.9998179078429128,\n",
       " 0.99977791969705621,\n",
       " 0.99938808737360163,\n",
       " 0.99839042991253391,\n",
       " 0.99972701163871092,\n",
       " 0.99983678781276952,\n",
       " 0.99985496368264448,\n",
       " 0.99970398841547436,\n",
       " 0.99975644438093214,\n",
       " 0.99962076725795868,\n",
       " 0.9995843414851916,\n",
       " 0.99856109517352476,\n",
       " 0.99857048842934948,\n",
       " 0.99961291544635411,\n",
       " 0.99880972598887419,\n",
       " 0.99969196090970747,\n",
       " 0.99981486327162783,\n",
       " 0.99939092220927583,\n",
       " 0.99983200885788903,\n",
       " 0.99974297265252265,\n",
       " 0.99616807836781529,\n",
       " 0.99988859252241713,\n",
       " 0.99815596839171872,\n",
       " 0.9998067395879745,\n",
       " 0.99976539918834995,\n",
       " 0.99979329373157566,\n",
       " 0.99976954276439089,\n",
       " 0.9998798165462337,\n",
       " 0.99972374620394333,\n",
       " 0.99964043096988831,\n",
       " 0.99986611864651742,\n",
       " 0.99875925322551939,\n",
       " 0.9998650713732502,\n",
       " 0.99984406954783289,\n",
       " 0.99743801271289101,\n",
       " 0.99909460976937337,\n",
       " 0.97846001790427239,\n",
       " 0.99906512944461145,\n",
       " 0.99946884605965913,\n",
       " 0.99837755625105695,\n",
       " 0.99705437336720892,\n",
       " 0.99889207283239267,\n",
       " 0.99609352281453856,\n",
       " 0.99914005711726273,\n",
       " 0.99965329565234573,\n",
       " 0.99980434564811027,\n",
       " 0.99957038133024978,\n",
       " 0.99983135729246353,\n",
       " 0.99979277876788664,\n",
       " 0.99931753902101061,\n",
       " 0.99984350869422967,\n",
       " 0.99899363599971047,\n",
       " 0.99976130756950221,\n",
       " 0.99915240781216885,\n",
       " 0.99819259596923449,\n",
       " 0.99968231975579835,\n",
       " 0.99844841320116395,\n",
       " 0.99982420370831637,\n",
       " 0.99957662025403438,\n",
       " 0.99967012764778596,\n",
       " 0.99830338281533948,\n",
       " 0.99965612579039675,\n",
       " 0.99981453572496548,\n",
       " 0.9986228888181371,\n",
       " 0.99920029132171617,\n",
       " 0.99966571712565677,\n",
       " 0.99982335609288053,\n",
       " 0.99956214293271106,\n",
       " 0.99954369427275758,\n",
       " 0.9981368441584193,\n",
       " 0.99977469866630064,\n",
       " 0.99962400203209545,\n",
       " 0.99981066751258929,\n",
       " 0.99957985656903936,\n",
       " 0.99977975983146894,\n",
       " 0.99962534365462141,\n",
       " 0.99952459547774675,\n",
       " 0.99962568861956846,\n",
       " 0.99969485527398483,\n",
       " 0.99943616969325799,\n",
       " 0.99930412846149619,\n",
       " 0.9994356127373657,\n",
       " 0.99961999496045562,\n",
       " 0.99980790588509028,\n",
       " 0.99968292201032993,\n",
       " 0.99101682431828064,\n",
       " 0.99901391148266905,\n",
       " 0.99943308355918736,\n",
       " 0.99942879373094384,\n",
       " 0.99945445520709175,\n",
       " 0.99966757464693401,\n",
       " 0.99882089411204722,\n",
       " 0.99891871819703359,\n",
       " 0.99950463018285318,\n",
       " 0.99941247915987486,\n",
       " 0.99926199429504503,\n",
       " 0.99973863284948639,\n",
       " 0.99972483833628556,\n",
       " 0.9996799585575924,\n",
       " 0.99982362174078765,\n",
       " 0.99942487399486024,\n",
       " 0.99956409212453246,\n",
       " -0.9722673020383964,\n",
       " 0.99977331804745506,\n",
       " 0.99977725253272787,\n",
       " 0.99774737206139419,\n",
       " 0.99972751893881795,\n",
       " 0.99900372536069415,\n",
       " 0.99973962304316066,\n",
       " 0.99884197751040105,\n",
       " 0.99962033156679375,\n",
       " 0.99934809364660904,\n",
       " 0.99773732473527976,\n",
       " 0.9998058040430865,\n",
       " 0.99965080830761965,\n",
       " 0.99952330730973948,\n",
       " 0.99957422549913832,\n",
       " 0.99976603980033563,\n",
       " 0.9997912483668302,\n",
       " 0.99975488220723063,\n",
       " 0.99985651798705644,\n",
       " 0.99978597780542255,\n",
       " 0.98741489042641661,\n",
       " 0.99976459604289569,\n",
       " 0.99967550307472353,\n",
       " 0.99894775889820764,\n",
       " 0.99892779752522809,\n",
       " 0.99193926543201683,\n",
       " 0.99857326465190399,\n",
       " 0.99983053993418769,\n",
       " 0.99952419030743034,\n",
       " 0.99973979389462531,\n",
       " 0.998908096940894,\n",
       " 0.99984518082840379,\n",
       " 0.99979654493874404,\n",
       " 0.99980861918496566,\n",
       " 0.99930391866438162,\n",
       " 0.99929437191572601,\n",
       " 0.99759519942215324,\n",
       " 0.99940648194574289,\n",
       " 0.99920335477470867,\n",
       " 0.99941322912386255,\n",
       " 0.99719635320988065,\n",
       " -0.61397753005836686,\n",
       " 0.99932987559085906,\n",
       " 0.94304766802136297,\n",
       " 0.99881967920500236,\n",
       " 0.99974027533466081,\n",
       " 0.99959687804271546,\n",
       " 0.99969538392128321,\n",
       " 0.99817123226083382,\n",
       " 0.99947921375285442,\n",
       " 0.99787177130289573,\n",
       " 0.99337055024460552,\n",
       " 0.99772324461423201,\n",
       " 0.99945750113479004,\n",
       " 0.98892479955513402,\n",
       " 0.99962777525853808,\n",
       " 0.99893067109838973,\n",
       " 0.99980416586458032,\n",
       " 0.99936384147643409,\n",
       " 0.99806892631675181,\n",
       " 0.99926071376076719,\n",
       " 0.99898594522710726,\n",
       " 0.99970181102654732,\n",
       " 0.99976927701537288,\n",
       " 0.99896949558322579,\n",
       " 0.99920146011156807,\n",
       " 0.99957662080319687,\n",
       " 0.99961815759399064,\n",
       " 0.99899413266568982,\n",
       " 0.9997904925855664,\n",
       " 0.99972929143744782,\n",
       " 0.99725878822285674,\n",
       " 0.99939826518976727,\n",
       " 0.9998078397519089,\n",
       " 0.99944628621517539,\n",
       " 0.99940108326423904,\n",
       " 0.9992821766367771,\n",
       " 0.99947584234621512,\n",
       " 0.99969978307266827,\n",
       " 0.99857600307601535,\n",
       " 0.99843013610802411,\n",
       " 0.99480326492843429,\n",
       " 0.95417125341468911,\n",
       " 0.99931327462693043,\n",
       " 0.99933735485741837,\n",
       " 0.98542332684512146,\n",
       " 0.99979533284782351,\n",
       " 0.99089220060226535,\n",
       " 0.99969100617362772,\n",
       " 0.99957670582084179,\n",
       " 0.99324472704235578,\n",
       " 0.99868184482288758,\n",
       " 0.9997384242633709,\n",
       " 0.99975811935624836,\n",
       " 0.99980075019457981,\n",
       " 0.99973680434148893,\n",
       " 0.99958243251161183,\n",
       " 0.99947912629173219,\n",
       " -0.99234015292014865,\n",
       " 0.99965520690995913,\n",
       " 0.99980512578151781,\n",
       " 0.99977948116569282,\n",
       " 0.99942089066077899,\n",
       " 0.99981412067306408,\n",
       " 0.99972265387271286,\n",
       " 0.99884459772681788,\n",
       " 0.99979916411903791,\n",
       " 0.99953061615862682,\n",
       " 0.99970986902854764,\n",
       " 0.99941315424531074,\n",
       " 0.99962065936784816,\n",
       " 0.99983768828285535,\n",
       " 0.99954247163326415,\n",
       " 0.99970670037413378,\n",
       " 0.99973699056044552,\n",
       " 0.99405558541462891,\n",
       " 0.9998008393869916,\n",
       " 0.99929592774915632,\n",
       " 0.99954775733413725,\n",
       " 0.99980113444133156,\n",
       " 0.99959571069685338,\n",
       " 0.99957268209772598,\n",
       " 0.99947110926331306,\n",
       " 0.99981266459627094,\n",
       " 0.99975653572175882,\n",
       " 0.99982110450283534,\n",
       " 0.99772910761319034,\n",
       " 0.9998492073246571,\n",
       " 0.99945608231185834,\n",
       " 0.99950997781857187,\n",
       " 0.99979093982680256,\n",
       " 0.99948916673316179,\n",
       " 0.99981240124243542,\n",
       " 0.99821505577736991,\n",
       " 0.99929698688535473,\n",
       " 0.99922275486356693,\n",
       " 0.99979095964260301,\n",
       " 0.99492315834220546,\n",
       " 0.99529879967277413,\n",
       " 0.99970779011288657,\n",
       " 0.99970604013374209,\n",
       " 0.99980613411269026,\n",
       " 0.99972133313692624,\n",
       " 0.99977067927267871,\n",
       " 0.99979045550890788,\n",
       " 0.99814332844075471,\n",
       " 0.99932014915214595,\n",
       " 0.99962175791428198,\n",
       " -0.92049977297489205,\n",
       " 0.99983461899431858,\n",
       " 0.99953351707906568,\n",
       " 0.99931962172287503,\n",
       " 0.99954261998838956,\n",
       " 0.99980483822250421,\n",
       " 0.99943952078836251,\n",
       " 0.99945323233871963,\n",
       " 0.99987485506976626,\n",
       " 0.99967442639631399,\n",
       " 0.99944325623246133,\n",
       " -0.91697843649184907,\n",
       " 0.99888035857644975,\n",
       " 0.99967102768401006,\n",
       " 0.99405053177441705,\n",
       " 0.99964350501972077,\n",
       " 0.99966310857661034,\n",
       " 0.99913166541154841,\n",
       " 0.99963777248040042,\n",
       " 0.99962952731404675,\n",
       " 0.99941331386839249,\n",
       " 0.99969613397871482,\n",
       " 0.99443095318821639,\n",
       " 0.99984701910019735,\n",
       " 0.99985121547889266,\n",
       " 0.99963297846981902,\n",
       " 0.99982818405504992,\n",
       " 0.99973088715132019,\n",
       " 0.99571125338006561,\n",
       " 0.99978034367798851,\n",
       " 0.99420813889950821,\n",
       " 0.99863174007596078,\n",
       " 0.99869913754172446,\n",
       " 0.9930356074669584,\n",
       " 0.99956048998986291,\n",
       " 0.99982668353997184,\n",
       " 0.99940715725210494,\n",
       " 0.99956599689959336,\n",
       " 0.99968789197049612,\n",
       " 0.99978933026730143,\n",
       " 0.99958873725128339,\n",
       " 0.99922184754564047,\n",
       " 0.9997063939480425,\n",
       " 0.99958863941271481,\n",
       " 0.99952164967559987,\n",
       " 0.9884945191325627,\n",
       " -0.61748405153100328,\n",
       " 0.99951702407327958,\n",
       " 0.99719256399839196,\n",
       " 0.99964401914197221,\n",
       " 0.99933222199069982,\n",
       " 0.99912754755750244,\n",
       " 0.99978157855214334,\n",
       " 0.99981112684451046,\n",
       " 0.99980282245700203,\n",
       " 0.99954522760973608,\n",
       " 0.99968273128485829,\n",
       " 0.99957932739285005,\n",
       " 0.99892986729988864,\n",
       " 0.99966380377253061,\n",
       " 0.9991813860709331,\n",
       " 0.99405075069136006,\n",
       " 0.99982171742880743,\n",
       " 0.99880644084311665,\n",
       " 0.99963622550899311,\n",
       " 0.99962157967766496,\n",
       " 0.99945385540433884,\n",
       " 0.99101649977897743,\n",
       " 0.99923220188769235,\n",
       " 0.99944430533027628,\n",
       " 0.9991189914770704,\n",
       " 0.99879051359455684,\n",
       " 0.9996706714847523,\n",
       " 0.99937006075974155,\n",
       " 0.99988849048545048,\n",
       " 0.99916205903296706,\n",
       " 0.99882640364417141,\n",
       " 0.99881731171643451,\n",
       " 0.99985122892622458,\n",
       " 0.99969438160754465,\n",
       " 0.99981433996192914,\n",
       " 0.99895120893746081,\n",
       " 0.99983359328135957,\n",
       " 0.99912226349990474,\n",
       " 0.99511677149410005,\n",
       " 0.9997434294333829,\n",
       " 0.99968447924545623,\n",
       " 0.99934537141446511,\n",
       " 0.99961204829163186,\n",
       " 0.99973199761879139,\n",
       " 0.99986356780233021,\n",
       " 0.99989076540831234,\n",
       " 0.9987936834449207,\n",
       " 0.99974720671969675,\n",
       " 0.99957242690189174,\n",
       " 0.99959689637123272,\n",
       " 0.96841947131443473,\n",
       " 0.99955207215967701,\n",
       " 0.99976099434256072,\n",
       " 0.99981193784434008,\n",
       " 0.99913925257187242,\n",
       " 0.99947824328821189,\n",
       " 0.99942588017390288,\n",
       " 0.99979606653638808,\n",
       " 0.99991162273735912,\n",
       " 0.99973141917420105,\n",
       " 0.99797770521677531,\n",
       " 0.99947744957868156,\n",
       " 0.99856852687071718,\n",
       " 0.99985584937859673,\n",
       " 0.99980737897893779,\n",
       " 0.99915447016788861,\n",
       " 0.99981121393255878,\n",
       " 0.99967167314353933,\n",
       " 0.99972995727422842,\n",
       " 0.99793654866828896,\n",
       " 0.99926895940769245,\n",
       " 0.99930443268378777,\n",
       " 0.99933165743421604,\n",
       " 0.99945907749691654,\n",
       " 0.99982376488009728,\n",
       " 0.97326587965443434,\n",
       " 0.99988274167465296,\n",
       " 0.99976123266036865,\n",
       " 0.99765274664935355,\n",
       " 0.99973443402288575,\n",
       " -0.54403682099161499,\n",
       " 0.98669091124809205,\n",
       " -0.99933059367217836,\n",
       " 0.99972128770808799,\n",
       " 0.99988077773450978,\n",
       " 0.99979862076318571,\n",
       " 0.99980545085203132,\n",
       " 0.99980392760396852,\n",
       " 0.99973593288644058,\n",
       " 0.99964784783514726,\n",
       " 0.99979445724564231,\n",
       " 0.99980605431389391,\n",
       " 0.99919344736617632,\n",
       " 0.9982522139712765,\n",
       " 0.98586526855111389,\n",
       " 0.99844987288782994,\n",
       " 0.99989295260024769,\n",
       " 0.99950161816348515,\n",
       " 0.99980280112299247,\n",
       " 0.98512306753771883,\n",
       " 0.99921584824082832,\n",
       " 0.99817739706066677,\n",
       " 0.99856823254549865,\n",
       " 0.99977979809667339,\n",
       " 0.99975528799337432,\n",
       " 0.99934954718103419,\n",
       " 0.99941458624709878,\n",
       " 0.99891708735920859,\n",
       " 0.99925226520169441,\n",
       " 0.99938488806199344,\n",
       " 0.99975703165799834,\n",
       " 0.99981200373649581,\n",
       " 0.99929901947218702,\n",
       " 0.99972312024390708,\n",
       " 0.99963588777779722,\n",
       " 0.99915697821136962,\n",
       " 0.99955463536705946,\n",
       " 0.99964676375820871,\n",
       " 0.99934549279506013,\n",
       " 0.99746360919932875,\n",
       " 0.99911880727324265,\n",
       " 0.99723022799704764,\n",
       " 0.99975366247388298,\n",
       " 0.99913852594944597,\n",
       " 0.9997522544188977,\n",
       " 0.99965732080742753,\n",
       " 0.99935870252913939,\n",
       " 0.99960195506424054,\n",
       " 0.99979682215620569,\n",
       " 0.99962266772427455,\n",
       " 0.99947937379877405,\n",
       " 0.99979136013849446,\n",
       " 0.99943954900340171,\n",
       " 0.99864359222991028,\n",
       " 0.97911008032714808,\n",
       " 0.99979231682514902,\n",
       " 0.99694928785091519,\n",
       " 0.99953004398308043,\n",
       " 0.99970115863211795,\n",
       " 0.99978530107233665,\n",
       " 0.99932458095654841,\n",
       " 0.99958627512356801,\n",
       " 0.99984316269366191,\n",
       " 0.9998176597762225,\n",
       " 0.99903013338902114,\n",
       " 0.99934848404528709,\n",
       " 0.99969134339773458,\n",
       " 0.99930654775557026,\n",
       " 0.98878387222436381,\n",
       " 0.9994384736218429,\n",
       " 0.99790650978071349,\n",
       " 0.99974183578160947,\n",
       " 0.9997557255856675,\n",
       " 0.99942130560266607,\n",
       " 0.99958415511529231,\n",
       " 0.99841670109369529,\n",
       " 0.99957136668120283,\n",
       " -0.96232939418431451,\n",
       " 0.99965105988719483,\n",
       " 0.9997493110118304,\n",
       " 0.99979498908575792,\n",
       " 0.99896047623872464,\n",
       " 0.99974084660940632,\n",
       " 0.99966302084975722,\n",
       " 0.99980459774594832,\n",
       " 0.99870646560292342,\n",
       " 0.99717217911453737,\n",
       " 0.99986475353032445,\n",
       " 0.99949370626096912,\n",
       " 0.34064640407183022,\n",
       " 0.99950224334319038,\n",
       " -0.88949742122562714,\n",
       " 0.99969954527381044,\n",
       " -0.94189282644276551,\n",
       " 0.9997009880969252,\n",
       " 0.99982504506839198,\n",
       " 0.99982138447795177,\n",
       " 0.99957819781997026,\n",
       " 0.99984683399331797,\n",
       " 0.99963465018218367,\n",
       " 0.99480979039948136,\n",
       " 0.99978939592304106,\n",
       " 0.99918012403333711,\n",
       " 0.99974588021330912,\n",
       " 0.99927949394672888,\n",
       " 0.99920739968169203,\n",
       " 0.99946584672345185,\n",
       " 0.99903775172785503,\n",
       " 0.99935561919091376,\n",
       " 0.99818526535266583,\n",
       " 0.9997431003974796,\n",
       " 0.99965030146782252,\n",
       " 0.99904180710973456,\n",
       " 0.99977947721483784,\n",
       " 0.99979900634682106,\n",
       " 0.99967664600143835,\n",
       " 0.99976614423752819,\n",
       " 0.99981909278885484,\n",
       " 0.9997100353907985,\n",
       " 0.99929368665655605,\n",
       " 0.99975590429427708,\n",
       " 0.99907215901878355,\n",
       " 0.99918394010120537,\n",
       " 0.9992662552963868,\n",
       " 0.99985354386312941,\n",
       " 0.9997441908599759,\n",
       " 0.99950193595479586,\n",
       " 0.99971937082895312,\n",
       " 0.99980896832055444,\n",
       " 0.99984271689836579,\n",
       " 0.99830363643123499,\n",
       " 0.99878490690755439,\n",
       " 0.99976993002588843,\n",
       " 0.98397455394230904,\n",
       " 0.035950230125941611,\n",
       " 0.99886851931011023,\n",
       " 0.9998005727870356,\n",
       " 0.99970241539690019,\n",
       " 0.99953602119246587,\n",
       " 0.99972669099825739,\n",
       " 0.99985843842837685,\n",
       " 0.99960574804731528,\n",
       " 0.99967597495495297,\n",
       " 0.99977206031432642,\n",
       " 0.99964346567206763,\n",
       " 0.99919372098595416,\n",
       " 0.99972738896915869,\n",
       " 0.99923548715843147,\n",
       " 0.99963925642091678,\n",
       " 0.99984813736260003,\n",
       " 0.99648209240940866,\n",
       " 0.99945048564808836,\n",
       " 0.99974190888414338,\n",
       " 0.9994939604635088,\n",
       " 0.99654490161789577,\n",
       " 0.99760298573867534,\n",
       " 0.99950694970035125,\n",
       " 0.9992823992899732,\n",
       " 0.99977998019345005,\n",
       " 0.99945385656987706,\n",
       " 0.99974953901690677,\n",
       " 0.99634509969918506,\n",
       " 0.99970957301895047,\n",
       " 0.99984512612612209,\n",
       " 0.99708112159677165,\n",
       " 0.99965842292292462,\n",
       " 0.99930867133892776,\n",
       " 0.99831165034811642,\n",
       " 0.97472772090943183,\n",
       " 0.9995712057701337,\n",
       " 0.99956000521590371,\n",
       " 0.99968716492263476,\n",
       " 0.9996642924625696,\n",
       " 0.99785607581750424,\n",
       " 0.99965964153386411,\n",
       " 0.99343889666416652,\n",
       " 0.99966332851366846,\n",
       " 0.99659489947370083,\n",
       " 0.99889769443793308,\n",
       " 0.99970051785685554,\n",
       " 0.99953937119739134,\n",
       " 0.99969952677669593,\n",
       " 0.99960352334413982,\n",
       " 0.96340168414092475,\n",
       " 0.99649891462330309,\n",
       " 0.99855205686144533,\n",
       " 0.99985406100710827,\n",
       " 0.99945121401973791,\n",
       " 0.99983566492864939,\n",
       " 0.99243940618673543,\n",
       " 0.99877318665140391,\n",
       " 0.99890587049934432,\n",
       " 0.99970920544395159,\n",
       " 0.99986760512458794,\n",
       " 0.99355220625259244,\n",
       " 0.99971513742970242,\n",
       " 0.99945383019808109,\n",
       " 0.9998648107362097,\n",
       " 0.99937567970934293,\n",
       " 0.99097649813088851,\n",
       " 0.9996669479351612,\n",
       " 0.99950099666208703,\n",
       " 0.99953869360448522,\n",
       " 0.99968749448687799,\n",
       " 0.99979040782419648,\n",
       " 0.99933379767964436,\n",
       " 0.99950404246359059,\n",
       " 0.99977850682006897,\n",
       " 0.99957920148943546,\n",
       " 0.9995566633272881,\n",
       " 0.99972223043306663,\n",
       " 0.99976721057951756,\n",
       " 0.99944520889062871,\n",
       " 0.99983580442584086,\n",
       " 0.99987586386902427,\n",
       " 0.999548659518396]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindf[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
