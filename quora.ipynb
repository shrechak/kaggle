{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download('punkt')\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words(question) :\n",
    "    return list(filter(lambda word: word not in stop and word.isalpha(), nltk.word_tokenize(question.lower())))\n",
    "\n",
    "getLabeledSentence = lambda given_tuple: [\n",
    "             LabeledSentence(get_words(given_tuple[0]),[\"question1_\"+str(given_tuple[2])]),  \n",
    "             LabeledSentence(get_words(given_tuple[1]),[\"question2_\"+str(given_tuple[2])])]\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 7)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf = pd.read_csv('train.csv')\n",
    "traindf['index1'] = traindf.index\n",
    "traindf = traindf.fillna(\"\")\n",
    "traindf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 6)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf = pd.read_csv('train.csv')\n",
    "testdf = testdf.fillna(\"\")\n",
    "testdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindf = traindf[:1000]\n",
    "testdf = testdf[:1000]\n",
    "\n",
    "train_df_size = traindf.shape[0]\n",
    "test_df_size = testdf.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainSentences = flatten(list(map(lambda x: getLabeledSentence(x) , traindf[['question1', 'question2', 'id']].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testSentences = flatten(list(map(lambda x: getLabeledSentence(x) , testdf[['question1', 'question2', 'id']].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = trainSentences + testSentences\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=1000, sample=1e-4, negative=5, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-21 16:40:21,032 : INFO : collecting all words and their counts\n",
      "2017-04-21 16:40:21,034 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-04-21 16:40:21,080 : INFO : collected 3594 word types and 2000 unique tags from a corpus of 4000 examples and 20460 words\n",
      "2017-04-21 16:40:21,081 : INFO : Loading a fresh vocabulary\n",
      "2017-04-21 16:40:21,096 : INFO : min_count=1 retains 3594 unique words (100% of original 3594, drops 0)\n",
      "2017-04-21 16:40:21,097 : INFO : min_count=1 leaves 20460 word corpus (100% of original 20460, drops 0)\n",
      "2017-04-21 16:40:21,145 : INFO : deleting the raw counts dictionary of 3594 items\n",
      "2017-04-21 16:40:21,146 : INFO : sample=0.0001 downsamples 946 most-common words\n",
      "2017-04-21 16:40:21,147 : INFO : downsampling leaves estimated 13709 word corpus (67.0% of prior 20460)\n",
      "2017-04-21 16:40:21,148 : INFO : estimated required memory for 3594 words and 1000 dimensions: 38949000 bytes\n",
      "2017-04-21 16:40:21,174 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-21 16:40:22,738 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-21 16:40:22,740 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-21 16:40:24,276 : INFO : PROGRESS: at 9.79% examples, 5677 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-21 16:40:24,323 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-21 16:40:24,329 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-21 16:40:24,339 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-21 16:40:24,343 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-21 16:40:24,363 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-21 16:40:24,459 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-21 16:40:24,711 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-21 16:40:24,743 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-21 16:40:24,744 : INFO : training on 102300 raw words (88570 effective words) took 2.0s, 44643 effective words/s\n",
      "2017-04-21 16:40:24,745 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-21 16:40:24,751 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-21 16:40:24,755 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-21 16:40:25,871 : INFO : PROGRESS: at 9.72% examples, 7830 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-21 16:40:25,918 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-21 16:40:25,943 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-21 16:40:25,948 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-21 16:40:25,958 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-21 16:40:25,964 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-21 16:40:26,040 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-21 16:40:26,262 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-21 16:40:26,282 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-21 16:40:26,283 : INFO : training on 102300 raw words (88570 effective words) took 1.5s, 58676 effective words/s\n",
      "2017-04-21 16:40:26,284 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-21 16:40:26,289 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-21 16:40:26,290 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-21 16:40:27,874 : INFO : PROGRESS: at 9.80% examples, 5595 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-21 16:40:27,966 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-21 16:40:28,016 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-21 16:40:28,048 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-21 16:40:28,058 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-21 16:40:28,068 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-21 16:40:28,151 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-21 16:40:28,366 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-21 16:40:28,368 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-21 16:40:28,368 : INFO : training on 102300 raw words (88481 effective words) took 2.0s, 43190 effective words/s\n",
      "2017-04-21 16:40:28,369 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-21 16:40:28,374 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-21 16:40:28,375 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-21 16:40:29,921 : INFO : PROGRESS: at 9.76% examples, 5654 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-21 16:40:29,956 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-21 16:40:29,970 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-21 16:40:29,992 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-21 16:40:30,024 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-21 16:40:30,050 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-21 16:40:30,166 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-21 16:40:30,428 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-21 16:40:30,486 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-21 16:40:30,487 : INFO : training on 102300 raw words (88391 effective words) took 2.1s, 42083 effective words/s\n",
      "2017-04-21 16:40:30,488 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-21 16:40:30,493 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-21 16:40:30,494 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-21 16:40:31,888 : INFO : PROGRESS: at 9.75% examples, 6251 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-21 16:40:31,968 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-21 16:40:31,989 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-21 16:40:32,014 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-21 16:40:32,063 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-21 16:40:32,067 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-21 16:40:32,176 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-21 16:40:32,599 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-21 16:40:32,652 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-21 16:40:32,653 : INFO : training on 102300 raw words (88745 effective words) took 2.2s, 41252 effective words/s\n",
      "2017-04-21 16:40:32,653 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-21 16:40:32,659 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-21 16:40:32,661 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-21 16:40:34,463 : INFO : PROGRESS: at 9.84% examples, 4886 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-21 16:40:34,494 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-21 16:40:34,502 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-21 16:40:34,511 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-21 16:40:34,514 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-21 16:40:34,543 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-21 16:40:34,610 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-21 16:40:34,835 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-21 16:40:34,845 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-21 16:40:34,846 : INFO : training on 102300 raw words (88352 effective words) took 2.2s, 40585 effective words/s\n",
      "2017-04-21 16:40:34,847 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-21 16:40:34,856 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-21 16:40:34,860 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-21 16:40:36,425 : INFO : PROGRESS: at 9.70% examples, 5630 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-21 16:40:36,478 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-21 16:40:36,511 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-21 16:40:36,527 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-21 16:40:36,538 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-21 16:40:36,594 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-21 16:40:36,687 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-21 16:40:36,919 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-21 16:40:36,944 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-21 16:40:36,944 : INFO : training on 102300 raw words (88456 effective words) took 2.1s, 42969 effective words/s\n",
      "2017-04-21 16:40:36,945 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-21 16:40:36,950 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-21 16:40:36,951 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-21 16:40:38,311 : INFO : PROGRESS: at 9.82% examples, 6395 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-21 16:40:38,383 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-21 16:40:38,396 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-21 16:40:38,406 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-21 16:40:38,419 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-21 16:40:38,447 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-21 16:40:38,571 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-21 16:40:38,826 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-21 16:40:38,835 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-21 16:40:38,836 : INFO : training on 102300 raw words (88601 effective words) took 1.9s, 47258 effective words/s\n",
      "2017-04-21 16:40:38,838 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-21 16:40:38,845 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-21 16:40:38,845 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-21 16:40:40,760 : INFO : PROGRESS: at 9.78% examples, 4590 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-21 16:40:40,803 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-21 16:40:40,807 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-21 16:40:40,830 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-21 16:40:40,838 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-21 16:40:40,885 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-21 16:40:41,072 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-21 16:40:41,260 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-21 16:40:41,267 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-21 16:40:41,268 : INFO : training on 102300 raw words (88522 effective words) took 2.4s, 36837 effective words/s\n",
      "2017-04-21 16:40:41,269 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-21 16:40:41,275 : INFO : training model with 8 workers on 3594 vocabulary and 1000 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-04-21 16:40:41,278 : INFO : expecting 4000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-21 16:40:42,518 : INFO : PROGRESS: at 9.68% examples, 7053 words/s, in_qsize 10, out_qsize 0\n",
      "2017-04-21 16:40:42,555 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-04-21 16:40:42,558 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-21 16:40:42,571 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-21 16:40:42,583 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-21 16:40:42,593 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-21 16:40:42,664 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-21 16:40:42,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-21 16:40:42,933 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-21 16:40:42,934 : INFO : training on 102300 raw words (88580 effective words) took 1.6s, 53971 effective words/s\n",
      "2017-04-21 16:40:42,935 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    random.shuffle(sentences)\n",
    "    model.train(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "yaxis = []\n",
    "for i in range(0, train_df_size):\n",
    "    x = get_words(traindf.iloc[i].question1)\n",
    "    y = get_words(traindf.iloc[i].question2)\n",
    "    if len(x) > 0 and len(y) > 0:\n",
    "        sim = model.n_similarity(x,y)\n",
    "    else:\n",
    "        sim = 0\n",
    "    train_data.append(sim)\n",
    "    train_labels.append(traindf.iloc[i].is_duplicate)\n",
    "    yaxis.append(i)\n",
    "    \n",
    "print(len(train_data))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngwx3SWqQ4S5JDTLcJalBhrskNeh/AVZtYZu3v+MLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f443393c048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(yaxis, train_data, \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = np.array(train_data[:500]).reshape(-1,1)\n",
    "test_labels = np.array(train_labels[:500]).reshape(-1,1)\n",
    "train_data = np.array(train_data).reshape(-1,1)\n",
    "train_labels = np.array(train_labels).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfNN = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/indix/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:904: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfNN.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.624"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfNN.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testId = []\n",
    "test_similarity = []\n",
    "for i in range(0, test_df_size):\n",
    "    x = get_words(testdf.iloc[i].question1)\n",
    "    y = get_words(testdf.iloc[i].question2)\n",
    "    if len(x) > 0 and len(y) > 0:\n",
    "        sim = model.n_similarity(x,y)\n",
    "    else:\n",
    "        sim = 0\n",
    "    testId.append(i)\n",
    "    test_similarity.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99999423775390306,\n",
       " 0.99997759268055209,\n",
       " 0.99998858746352914,\n",
       " 0.9999585412607227,\n",
       " 0.99999146344523981,\n",
       " 0.99999673049152416,\n",
       " 0.99995398363802313,\n",
       " 0.99993890611517255,\n",
       " 0.99999686597531279,\n",
       " 0.99998876816180449,\n",
       " 0.99997121634044728,\n",
       " 0.99998973226748533,\n",
       " 1.0000000000000002,\n",
       " 0.99999652512791715,\n",
       " 0.99999895756390467,\n",
       " 0.99999087085595484,\n",
       " 0.99995758755412656,\n",
       " 0.99998487735408048,\n",
       " 0.99999597657144368,\n",
       " 0.9999933028407213,\n",
       " 0.99999243769342705,\n",
       " 0.99999233824079181,\n",
       " 0.99999081038744309,\n",
       " 0.99996477141238738,\n",
       " 0.99998546540985478,\n",
       " 0.99999774756392135,\n",
       " 0.99999503235564402,\n",
       " 0.99998608478215423,\n",
       " 0.99999697942607157,\n",
       " 0.99999333841044125,\n",
       " 0.99999453226316437,\n",
       " 0.9999928705231117,\n",
       " 0.99999710671513675,\n",
       " 0.99994616980417927,\n",
       " 0.99999846739911091,\n",
       " 0.99999191962443679,\n",
       " 0.99999213998411518,\n",
       " 0.99998068106254467,\n",
       " 0.99997156598833803,\n",
       " 0.99998445975448813,\n",
       " 0.99991530086872948,\n",
       " 1.0000000000000002,\n",
       " 1.0000000000000002,\n",
       " 0.99998768647995018,\n",
       " 0.99999301949538599,\n",
       " 0.99999550095594347,\n",
       " 0.99994641874265666,\n",
       " 0.99994160119240683,\n",
       " 0.99999200740590843,\n",
       " 1.0000000000000002,\n",
       " 0.99999661326408673,\n",
       " 0.99999946227136693,\n",
       " 0.99996109381613152,\n",
       " 0.99998615587674344,\n",
       " 0.99995203584013603,\n",
       " 0.99998443012978533,\n",
       " 0.99997299726354483,\n",
       " 0.99999191462180226,\n",
       " 0.99999636923292856,\n",
       " 0.99998564003809387,\n",
       " 0.99997813974461847,\n",
       " 0.99999559742975497,\n",
       " 0.99997523626685825,\n",
       " 0.99999871202940205,\n",
       " 0.99999592047748698,\n",
       " 0.99998885646612312,\n",
       " 0.9999933514891417,\n",
       " 0.99999497274974092,\n",
       " 0.99998741358919851,\n",
       " 0.99996608058807646,\n",
       " 0.99998317941082504,\n",
       " 0.99999999999999978,\n",
       " 0.99999287622499111,\n",
       " 0.99998924092504959,\n",
       " 0.99999861689101899,\n",
       " 0.99987801006675225,\n",
       " 0.99999316823061568,\n",
       " 0.99998799420015971,\n",
       " 0.99999122135595164,\n",
       " 0.99999191337139881,\n",
       " 0.99997215159479802,\n",
       " 0.99998739366059364,\n",
       " 0.99997641467863507,\n",
       " 0.99997958950833477,\n",
       " 0.99999096491892014,\n",
       " 0.99999314766460978,\n",
       " 0.99999920240972251,\n",
       " 0.99998636367924709,\n",
       " 0.99998950918555662,\n",
       " 1.0000000000000002,\n",
       " 0.99999507576867197,\n",
       " 0.99999508933070225,\n",
       " 0.9999917667734094,\n",
       " 0.99998367113394071,\n",
       " 0.9999745973723142,\n",
       " 0.99999999999999989,\n",
       " 0.99998785282094571,\n",
       " 0.99986903795847082,\n",
       " 0.99999084458232224,\n",
       " 0.99998365782303922,\n",
       " 0.99998919431585898,\n",
       " 0.9999935238201465,\n",
       " 0.99999402820314942,\n",
       " 0.99999403184058455,\n",
       " 0.99999646989147206,\n",
       " 0.99998514809065353,\n",
       " 0.99993286950301707,\n",
       " 1.0,\n",
       " 0.99999933534977359,\n",
       " 0.9999801866099387,\n",
       " 0.99998777740867617,\n",
       " 0.99999999999999956,\n",
       " 0.99998693873043454,\n",
       " 0.9999863166060009,\n",
       " 0.99999219401755624,\n",
       " 0.99997337790647811,\n",
       " 0.9999890576582221,\n",
       " 0.99998056103930411,\n",
       " 0.99999624665827236,\n",
       " 0.99998262012123984,\n",
       " 0.99997887330360058,\n",
       " 0.99998556109597059,\n",
       " 0.99999530142039217,\n",
       " 0.99996934585322628,\n",
       " 0.99994470722455586,\n",
       " 0.99999294941815742,\n",
       " 0.99998751625287108,\n",
       " 0.99999836953040067,\n",
       " 0.99998890604337898,\n",
       " 0.99999799812744716,\n",
       " 0.99987612254071789,\n",
       " 0.99996453670283592,\n",
       " 0.99998129083728382,\n",
       " 0.99999516355523244,\n",
       " 0.9999908877688487,\n",
       " 0.99997703786163139,\n",
       " 0.99999537307770159,\n",
       " 0.99999639120093953,\n",
       " 0.99999221086007317,\n",
       " 0.99999523176248695,\n",
       " 0.99998517300584577,\n",
       " 0.99998687417336563,\n",
       " 0.99996553321881709,\n",
       " 0.99999896463468285,\n",
       " 0.99998758271297827,\n",
       " 0.99999939812707106,\n",
       " 0.99998292158670543,\n",
       " 0.99997771089697085,\n",
       " 0.99999157511007064,\n",
       " 0.99998840770991992,\n",
       " 0.99997941137104862,\n",
       " 0.99998995076429797,\n",
       " 0.9999911940996552,\n",
       " 0.99999459552829695,\n",
       " -0.058281971777718572,\n",
       " 0.99999540805134801,\n",
       " 0.99999001032370982,\n",
       " 0.9999841662968435,\n",
       " 0.99999023054316027,\n",
       " 0.99998926296310486,\n",
       " 0.9999874554032202,\n",
       " 0.99997987299927715,\n",
       " 0.99998840866716576,\n",
       " 0.99999720656040514,\n",
       " 0.99999693792393041,\n",
       " 0.99996013574333287,\n",
       " 0.999978902787938,\n",
       " 0.99999108597475206,\n",
       " 0.9999923264493874,\n",
       " 0.99998787483905038,\n",
       " 0.99998635841671579,\n",
       " 0.99997849270756178,\n",
       " 0.99994656108367197,\n",
       " 0.99997032858309987,\n",
       " 0.99999999999999856,\n",
       " 0.99999999999999978,\n",
       " 0.99999390084609707,\n",
       " 0.99999827653631568,\n",
       " 0.99999376896722236,\n",
       " 0.99999245500952849,\n",
       " 0.99998882361820174,\n",
       " 0.99998955242859333,\n",
       " 0.9999903024219402,\n",
       " 0.99998303073039085,\n",
       " 0.99998667508235028,\n",
       " 0.99999970812098982,\n",
       " 0.99998766009874163,\n",
       " 0.999982345104932,\n",
       " 0.99999467199901781,\n",
       " 0.99999786354893827,\n",
       " 0.99999532843708405,\n",
       " 0.9999817447301842,\n",
       " 0.9999909702232318,\n",
       " 0.99999203195645892,\n",
       " 0.99998332116839028,\n",
       " 0.99999999999999833,\n",
       " 0.99996659108349062,\n",
       " 0.99999911718868284,\n",
       " 1.0000000000000002,\n",
       " 0.99998463056950493,\n",
       " 0.99999382821409055,\n",
       " 0.99998013375897343,\n",
       " 0.99999678418334492,\n",
       " 0.999994515090457,\n",
       " 0.99997987773767727,\n",
       " 0.99999624318799818,\n",
       " 0.99999999999999911,\n",
       " 0.9999941407685744,\n",
       " 0.99999223741027232,\n",
       " 0.99999999999999967,\n",
       " 0.99999031063601629,\n",
       " 0.99996472552297933,\n",
       " 0.99994254770414837,\n",
       " 0.99999592914327684,\n",
       " 0.99996145824453941,\n",
       " 0.99999871702807874,\n",
       " 0.9999884405735987,\n",
       " 0.99999271071742679,\n",
       " 0.99996063212171582,\n",
       " 0.99998468154332287,\n",
       " 0.99999236608633069,\n",
       " 0.99992530531474444,\n",
       " 0.99999999999999978,\n",
       " 0.99997050015622169,\n",
       " 0.99999042636628455,\n",
       " 0.99999212751639166,\n",
       " 0.99999900058128355,\n",
       " 0.99998389924129727,\n",
       " 0.99999405672310981,\n",
       " 0.99999716546957229,\n",
       " 0.99999296747283684,\n",
       " 0.99998641880880579,\n",
       " 0.99999975199208591,\n",
       " 0.99996995432497082,\n",
       " 0.99999269601714924,\n",
       " 0.99998657457240026,\n",
       " 0.99999283225807967,\n",
       " 0.99999139723595643,\n",
       " 0.99999429826480435,\n",
       " 0.99995871970873451,\n",
       " 0.99999999999999989,\n",
       " 0.9999780195705994,\n",
       " 0.99999314625470592,\n",
       " 0.99999209715633208,\n",
       " 0.99998508631684324,\n",
       " 0.99998617233268772,\n",
       " 0.99997535757443468,\n",
       " 0.99999618458951067,\n",
       " 0.999989125971673,\n",
       " 0.99999304025807068,\n",
       " 0.9999841427087317,\n",
       " 0.99999830209499085,\n",
       " 0.99999592640192081,\n",
       " 0.99999999999999989,\n",
       " 0.99999216566968141,\n",
       " 0.99998283523984743,\n",
       " 0.99998855371507622,\n",
       " 0.99999459250597722,\n",
       " 0.99997748878901838,\n",
       " 0.9999763454598477,\n",
       " 0.99999629994428174,\n",
       " 0.99998893663435562,\n",
       " 0.99999313542156254,\n",
       " 0.99998824314780266,\n",
       " 0.99992126807699622,\n",
       " 0.999993006757389,\n",
       " 0.99997379522422525,\n",
       " 0.99999259550316089,\n",
       " 0.99999076591146197,\n",
       " 0.99997299018480668,\n",
       " 1.0,\n",
       " 0.99998976136617668,\n",
       " 0.99995970143350532,\n",
       " 0.99999513722388622,\n",
       " 0.99999679751372639,\n",
       " 0.99999618531617485,\n",
       " 0.99998035499878901,\n",
       " 0.99999999999999989,\n",
       " 0.99999609844649529,\n",
       " 0.99998609187377185,\n",
       " 0.99996122233348916,\n",
       " 0.99999466552604532,\n",
       " 0.99999286563588741,\n",
       " 0.99991519558018138,\n",
       " 0.99999490471485464,\n",
       " 0.99999194338083264,\n",
       " 0.99998971910984447,\n",
       " 0.99999942929190999,\n",
       " 0.99998917399084519,\n",
       " 0.99999219306388887,\n",
       " 0.9999978536137083,\n",
       " 0.99999999999999878,\n",
       " 0,\n",
       " 0.99999043149524081,\n",
       " 0.99997976293436586,\n",
       " 0.99999074349025951,\n",
       " 0.99993829747459528,\n",
       " 0.99996874520512624,\n",
       " 1.0000000000000002,\n",
       " 0.99999999999999989,\n",
       " 0.99999791558745543,\n",
       " 0.99994133158457121,\n",
       " 0.99999999999999989,\n",
       " 0.99999999999999989,\n",
       " 0.99999570606084354,\n",
       " 0.99999252499803781,\n",
       " 0.99998729713358714,\n",
       " 0.99998860802848677,\n",
       " 0.99998804964990107,\n",
       " 0.99995218278435793,\n",
       " 0.99998306002795023,\n",
       " 0.99998407434842973,\n",
       " 0.99999658307070327,\n",
       " 0.99998503918515214,\n",
       " 0.99999114812209533,\n",
       " 0.99990106285695823,\n",
       " 0.99997319293936326,\n",
       " 0.99999999999999978,\n",
       " 0.99999636118492441,\n",
       " 1.0,\n",
       " 0.9999870380348187,\n",
       " 0.99999644596173698,\n",
       " 0.99999564407123676,\n",
       " 0.99998293096203617,\n",
       " 0.99999574498939392,\n",
       " 0.99998062167400659,\n",
       " 0.99996740412979046,\n",
       " 0.99999873999615541,\n",
       " 1.0,\n",
       " 0.99996890740985278,\n",
       " 0.99997902638125347,\n",
       " 0.99999494533076028,\n",
       " 0.99998713194391342,\n",
       " 0.9999782306761692,\n",
       " 0.99999522723954626,\n",
       " 0.9999908501105399,\n",
       " 0.99998377390885573,\n",
       " 0.99997617258589711,\n",
       " 0.9999912786355627,\n",
       " 0.99994577019069653,\n",
       " 0.99999215748585579,\n",
       " 0.99998608135397016,\n",
       " 0.99999414885839155,\n",
       " 0.99998892436061304,\n",
       " 0.99998694721850689,\n",
       " 0.99999005914673089,\n",
       " 0.99997802936024882,\n",
       " 0.99999434106337715,\n",
       " 0.99999226895851956,\n",
       " 0.99999664712322545,\n",
       " 0.99996254028078202,\n",
       " 0.99999821356732299,\n",
       " 0.99999803560017209,\n",
       " 0.99999279973740229,\n",
       " 0.99999936120762767,\n",
       " 0.99999123965281311,\n",
       " 0.99999643475399924,\n",
       " 0.99999337973371172,\n",
       " 0.99995801521298344,\n",
       " 0.99999999999999989,\n",
       " 0.9999618408671358,\n",
       " 0.99998561239462103,\n",
       " 0.99999223719162145,\n",
       " 0.99999081766784892,\n",
       " 0.9999894418818096,\n",
       " 0.99999291081480468,\n",
       " 0.99999142955987042,\n",
       " 0.99998706982408925,\n",
       " 0.99998013624598603,\n",
       " 0.99999920287490884,\n",
       " 0.99998101519249227,\n",
       " 0.99997301751013212,\n",
       " 0.99998961852322288,\n",
       " 0.99997643013773074,\n",
       " 0.99995675622096414,\n",
       " 0.99999532700910043,\n",
       " 0.99998091261173194,\n",
       " 1.0,\n",
       " 0.99999310746523962,\n",
       " 0.99998303144370593,\n",
       " 0.99998721999095141,\n",
       " 0.99997635248910732,\n",
       " 0.99998571587825791,\n",
       " 0.99999234562135797,\n",
       " 0.99998501745783186,\n",
       " 0.99998813645040885,\n",
       " 0.99995956532218866,\n",
       " 0.99998608102538178,\n",
       " 1.0000000000000002,\n",
       " 0.9999493337372064,\n",
       " 0.99998210873067084,\n",
       " 0.99995088271041543,\n",
       " 0.99999045438273326,\n",
       " 0.99999072036163295,\n",
       " 0.99998670069361428,\n",
       " 0.99994719046040903,\n",
       " 0.99998873778436015,\n",
       " 0.99999601686559136,\n",
       " 0.99997334658847592,\n",
       " 0.99999021123952392,\n",
       " 0.9999906425050491,\n",
       " 0.99998189467630993,\n",
       " 0.99999612247984293,\n",
       " 0.99997860896837831,\n",
       " 0.99999368402699196,\n",
       " 0.99998262704422403,\n",
       " 0.99998255750821741,\n",
       " 0.99995819563926658,\n",
       " 0.99999867447561164,\n",
       " 0.99996884869265268,\n",
       " 0.99998473405457766,\n",
       " 0.99996890835757146,\n",
       " 0.99999444876166155,\n",
       " 0.99995766322558344,\n",
       " 0.9999859441836999,\n",
       " 0.99999600178875514,\n",
       " 0.99997416620062685,\n",
       " 0.99997612504408706,\n",
       " 0.99997822868485997,\n",
       " 0.99998187944214822,\n",
       " 0.99998319972204663,\n",
       " 0.99999999999999978,\n",
       " 0.99998381468302711,\n",
       " 0.99999897971433871,\n",
       " 0.99998025560866111,\n",
       " 0.9999863081099265,\n",
       " 0.99998840528131772,\n",
       " 0.99999693492245789,\n",
       " 0.99999784065529851,\n",
       " 0.99997512948777156,\n",
       " 0.99997517652025758,\n",
       " 0.99999724171527438,\n",
       " 0.99999407333091461,\n",
       " 0.99996570440874866,\n",
       " 0.99999186776408255,\n",
       " 0.99995137817828417,\n",
       " 0.99998024670155472,\n",
       " 0.99998157416485345,\n",
       " 0.99999496676796695,\n",
       " 0.9999920193158891,\n",
       " 0.99997404032080439,\n",
       " 0.99997925654444042,\n",
       " 0.9999862547257492,\n",
       " 0.99997327182890516,\n",
       " 0.99998129289783177,\n",
       " 0.99999395026056348,\n",
       " 0.99999774038104572,\n",
       " 0.99998170957412102,\n",
       " 0.99999324993829897,\n",
       " 0.99997791558644844,\n",
       " 0.99999613564548562,\n",
       " 0.99999999999999989,\n",
       " 0.99997048300130165,\n",
       " 0.99998252311398972,\n",
       " 0.99999410645887121,\n",
       " 0.99996341280940382,\n",
       " 0.99998157328179815,\n",
       " 0.99998366443906583,\n",
       " 0.99999999999999989,\n",
       " 0.99996997239900298,\n",
       " 0.99999636075761567,\n",
       " 0.99999261102560921,\n",
       " 1.0000000000000002,\n",
       " 0.99999198932816824,\n",
       " 0.99998005236563892,\n",
       " 0.99997121615524609,\n",
       " 0.99996973353889351,\n",
       " 0.99998467849943418,\n",
       " 0.99997697773625527,\n",
       " 0.99998136917598368,\n",
       " 0.99995547514869088,\n",
       " 0.99994571123140252,\n",
       " 0.99996263116372486,\n",
       " 0.99999544753312453,\n",
       " 0.9999611928641019,\n",
       " 0.99999045754827243,\n",
       " 0.99999713290547065,\n",
       " 0.99999059842103799,\n",
       " 0.99997833737634423,\n",
       " 0.99999637462695934,\n",
       " 0.99997018509841873,\n",
       " 0.9999946669944737,\n",
       " 0.99997495037719164,\n",
       " 0.99999999999999989,\n",
       " 0.99999248321169409,\n",
       " 0.99996675190751716,\n",
       " 0.99998882167482872,\n",
       " 0.99999382539334647,\n",
       " 0.99999313766153353,\n",
       " 0.99999699056645985,\n",
       " 0.99997628255634075,\n",
       " 0.99998600425568951,\n",
       " 0.99999669386913703,\n",
       " 0.99998011192365843,\n",
       " 0.99999642936945865,\n",
       " 0.99995611496921477,\n",
       " 0.99999962620947469,\n",
       " 0.99988780921869813,\n",
       " 0.99998332518858568,\n",
       " 0.99997544284250917,\n",
       " 0.99999505214231432,\n",
       " 0.99999544734916024,\n",
       " 0.99999184594310797,\n",
       " 0.99998531166640547,\n",
       " 0.99998324642438841,\n",
       " 0.99998537658848297,\n",
       " 0.9999991627207313,\n",
       " 0.99999567670272604,\n",
       " 0.99997330788635597,\n",
       " 0.99998319170691163,\n",
       " 0.99998118025864013,\n",
       " 0.99980951951176633,\n",
       " 0.99987978451941351,\n",
       " 1.0,\n",
       " 0.9999787440542881,\n",
       " 0.99999507994058623,\n",
       " 0.99998945344533008,\n",
       " 0.99999032779051067,\n",
       " 0.99997115479387566,\n",
       " 0.99996793343664347,\n",
       " 0.99999486058251574,\n",
       " 0.99998177770290297,\n",
       " 0.99998811277875588,\n",
       " 0.99999669963241711,\n",
       " 0.99998626780372435,\n",
       " 0.99999081193314321,\n",
       " 0.99999574429151683,\n",
       " 0.99999831036947118,\n",
       " 0.99996464816584851,\n",
       " 0.99998985078107239,\n",
       " 0.99997093224942679,\n",
       " 0.99997167006619048,\n",
       " 0.99999031779202596,\n",
       " 1.0000000000000002,\n",
       " 0.99997073217948307,\n",
       " 0.99999532548062509,\n",
       " 0.99999712575099897,\n",
       " 0.99995934957116239,\n",
       " 0.99997999350844347,\n",
       " 0.99999274591062404,\n",
       " 0.99999630657019345,\n",
       " 0.9999910352152459,\n",
       " 0.99991807854165227,\n",
       " 0.99997892859076043,\n",
       " 0.99997837560245861,\n",
       " 0.9999930387096021,\n",
       " 0.99997775288587598,\n",
       " 0.99999561128616277,\n",
       " 0.99998247803014684,\n",
       " 0.99998982674292647,\n",
       " 0.99999171727510772,\n",
       " 0.99999745042536636,\n",
       " 0.99998465900237676,\n",
       " 0.99999203369685963,\n",
       " 0.99999204312774115,\n",
       " 0.99999417915005107,\n",
       " 0.99999431790173832,\n",
       " 0.99999999999999878,\n",
       " 0.99999751430853934,\n",
       " 0.99998031854139779,\n",
       " 0.99998780190170322,\n",
       " 0.99999237569498411,\n",
       " 0.99999241973237885,\n",
       " 0.99998480493996655,\n",
       " 0.99999187956571112,\n",
       " 0.99995769918293231,\n",
       " 0.99999127724250436,\n",
       " 0.99999321707162292,\n",
       " 0.99999999999999989,\n",
       " 0.99998217226188557,\n",
       " 0.99999422764769041,\n",
       " 0.99999781357633721,\n",
       " 0.99999424178695739,\n",
       " 0.9999991552891484,\n",
       " 0.99998014302205218,\n",
       " 0.9999990632360426,\n",
       " 0.99995840925066926,\n",
       " 0.99996083890455945,\n",
       " 0.9999876932735039,\n",
       " 0.99995447355923295,\n",
       " 0.99999246579091783,\n",
       " 0.99999382368621392,\n",
       " 0.99999048026790605,\n",
       " 0.99999142994536316,\n",
       " 0.9999962402221948,\n",
       " 1.0,\n",
       " 0.99999607852129269,\n",
       " 0.99996909047520344,\n",
       " 0.99998989218300227,\n",
       " 0.99999506645451286,\n",
       " 0.9999874943900614,\n",
       " 0.99995728322525845,\n",
       " 0.9999971846301442,\n",
       " 0.99999623305088947,\n",
       " 0.99999592877924959,\n",
       " 0.99999999999999967,\n",
       " 0.99999508368955192,\n",
       " 0.99998921125278561,\n",
       " 0.99997983774610943,\n",
       " 0.99999573077592829,\n",
       " 0.99998853281751998,\n",
       " 0.99999999999999989,\n",
       " 0.99999292606569279,\n",
       " 0.99997535333604914,\n",
       " 0.99997874702609824,\n",
       " 0.99999380146048977,\n",
       " 0.99997069970557917,\n",
       " 0.99999363125028762,\n",
       " 0.99998817806402429,\n",
       " 1.0,\n",
       " 0.99998494313316566,\n",
       " 0.99998057628111459,\n",
       " 0.99999083676707423,\n",
       " 0.99989729265471716,\n",
       " 0.99998788488670376,\n",
       " 0.99999099674871439,\n",
       " 0.99996523004595539,\n",
       " 1.0,\n",
       " 0.99998813013388077,\n",
       " 1.0,\n",
       " 0.99999374287691256,\n",
       " 0.9999990593322271,\n",
       " 0.99941300792708765,\n",
       " 0.9999895484923994,\n",
       " 0.99998743866420192,\n",
       " 0.99999288295028121,\n",
       " 0.99999853607191092,\n",
       " 0.99995699568579399,\n",
       " 0.99997509814055741,\n",
       " 0.9999994021761861,\n",
       " 0.99998410669508075,\n",
       " 0.9999689933561603,\n",
       " 0.99999433055327325,\n",
       " 0.99999167047823156,\n",
       " 0.99998652383537956,\n",
       " 0.9999932783921881,\n",
       " 0.99999410430042279,\n",
       " 0.99999763409703524,\n",
       " 0.99998453468694648,\n",
       " 0.99999956603219309,\n",
       " 0.99999905007445422,\n",
       " 0.99999509967874911,\n",
       " 0.99996562460202265,\n",
       " 0.99999483984682724,\n",
       " 0.99999478296394506,\n",
       " 0.999995123102838,\n",
       " 0.999979868095072,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99997093999561859,\n",
       " 0.99999218158103131,\n",
       " 0.99998619322452031,\n",
       " 0.99998882759235486,\n",
       " 0.99998825746372311,\n",
       " 0.99999218431275405,\n",
       " 0.99998973625504206,\n",
       " 0.99998934477695101,\n",
       " 0.99974885491273358,\n",
       " 0.99999999999999989,\n",
       " 0.99999289959263482,\n",
       " 0.9999921082631491,\n",
       " 0.99999324054046268,\n",
       " 0.99997798101272817,\n",
       " 0.99999528872464538,\n",
       " 0.99998693294092367,\n",
       " 1.0,\n",
       " 0.99998320718892209,\n",
       " 0.99997357660009734,\n",
       " 0.99999891155854392,\n",
       " 0.99999647331385078,\n",
       " 0.99998696544578836,\n",
       " 0.99998856578678874,\n",
       " 0.99999080914379213,\n",
       " 0.99997758183749375,\n",
       " 0.99999748481906403,\n",
       " 0.99996045065273131,\n",
       " 0.99998966886793894,\n",
       " 0.99999614527456548,\n",
       " 0.99999999999999878,\n",
       " 0.99994310183257251,\n",
       " 0.99999128967917217,\n",
       " 0.99999494818420509,\n",
       " 0.99994949692717905,\n",
       " 0.99998946682967438,\n",
       " 0.99997705872637255,\n",
       " 0.99999041822335544,\n",
       " 0.9999944971380631,\n",
       " 0.99999137817952188,\n",
       " 0.99999156631381214,\n",
       " 0.99996789389269136,\n",
       " 0.99997947099834594,\n",
       " 0.99999430154761348,\n",
       " 0.99998916974593699,\n",
       " 0.99994694959605057,\n",
       " 0.99998372102201183,\n",
       " 0.99999999999999989,\n",
       " 0.99999914019471781,\n",
       " 0.99998133089358454,\n",
       " 0.99999260647400345,\n",
       " 0.99999285522638437,\n",
       " 0.99999535477406698,\n",
       " 0.99998787150083335,\n",
       " 0.99998735618156298,\n",
       " 0.99999621580309783,\n",
       " 0.99999724578796823,\n",
       " 0.99999747512222292,\n",
       " 0.99998220319360531,\n",
       " 0.99999573758332427,\n",
       " 0.99998921499332649,\n",
       " 0.99999614792570224,\n",
       " 0.99996250697560218,\n",
       " 0.99996168502674987,\n",
       " 0.99999010803528721,\n",
       " 0.99999999999999989,\n",
       " 0.99998836373989564,\n",
       " 0.99999999999999978,\n",
       " 0.9999751892282589,\n",
       " 0.99998314581749626,\n",
       " 1.0,\n",
       " 0.99998431635163476,\n",
       " 0.99995500808791959,\n",
       " 0.99999140782951956,\n",
       " 0.99996847112351317,\n",
       " 0.99999486565286622,\n",
       " 0.99999699727104008,\n",
       " 0.99997028960089185,\n",
       " 0.99999031051320586,\n",
       " 0.99995894983371747,\n",
       " 0.99998980228669831,\n",
       " 0.9999872452969486,\n",
       " 0.99997924281318629,\n",
       " 0.99991889978446824,\n",
       " 0.99998157911352414,\n",
       " 0.999993181580207,\n",
       " 0.99998526470056182,\n",
       " 0.99998969378994085,\n",
       " 0.99995554371007378,\n",
       " 0.9999946633738902,\n",
       " 0.99997312396852067,\n",
       " 0.99999709724970809,\n",
       " 0.99997727671451719,\n",
       " 0.99999881375323751,\n",
       " 0.99999899408246717,\n",
       " 0.99998294653587427,\n",
       " 0.99999098669143649,\n",
       " 0.99998819247241844,\n",
       " 0.99999999999999978,\n",
       " 0.99998079168758558,\n",
       " 0.99999650004569507,\n",
       " 0.99998764276466079,\n",
       " 0.99999655070376114,\n",
       " 0.99997071918456193,\n",
       " 0.99999004571042049,\n",
       " 0.9999956832657737,\n",
       " 0.9999858310405203,\n",
       " 0.99999228162838227,\n",
       " 0.99998270804746858,\n",
       " 0.99998852517737946,\n",
       " 0.99995969294938436,\n",
       " 0.99998974283707232,\n",
       " 0.99998813170601608,\n",
       " 0.9999946749900257,\n",
       " 0.99996981217623282,\n",
       " 0.99996370127966783,\n",
       " 0.99999838332885194,\n",
       " 0.99998278586137279,\n",
       " 0.9999740902798111,\n",
       " 0.99993711718753386,\n",
       " 0.99999532570287419,\n",
       " 0.99999928683200456,\n",
       " 0.99998983780800144,\n",
       " 0.99998724314127196,\n",
       " 0.99999158117070674,\n",
       " 0.99998297604399822,\n",
       " 1.0000000000000002,\n",
       " 0.99996455748078505,\n",
       " 0.99999888145231408,\n",
       " 0.99999197069014467,\n",
       " 0.99999133513116456,\n",
       " 0.99999045221387051,\n",
       " 0.99998809042768189,\n",
       " 0.99999007622814862,\n",
       " 0.99998736387488141,\n",
       " 0.99997110405899314,\n",
       " 0.99999166148475904,\n",
       " 0.99997583951751601,\n",
       " 0.99996624365386866,\n",
       " 0.99999999999999989,\n",
       " 0.9999964670536845,\n",
       " 0.99999332670004126,\n",
       " 0.99999547781288289,\n",
       " 0.99997408715544622,\n",
       " 0.99999068750844655,\n",
       " 0.99998303276599565,\n",
       " 0.99998564659144562,\n",
       " 0.99999473493900481,\n",
       " 0.99999390182913084,\n",
       " 0.99998989850143805,\n",
       " 1.0,\n",
       " 0.99998915169649039,\n",
       " 0.99999967180439853,\n",
       " 0.99994028785545519,\n",
       " 0.99996240779730161,\n",
       " 0.99999630864167632,\n",
       " 0.99999185149555647,\n",
       " 0.99999999999999967,\n",
       " 0.99999331862468765,\n",
       " 0.99998717401380566,\n",
       " 0.99998369546053245,\n",
       " 0.99999603816074623,\n",
       " 0.99999568550344065,\n",
       " 0.99999295555047918,\n",
       " 0.99991645842772736,\n",
       " 0.99996968069334136,\n",
       " 0.99996044401630413,\n",
       " 0.99999824991235464,\n",
       " 0.99998603318443025,\n",
       " 0.99999999999999978,\n",
       " 0.99999169335735127,\n",
       " 0.99999452646138731,\n",
       " 0.99999342081221565,\n",
       " 0.99996690363336693,\n",
       " 1.0000000000000002,\n",
       " 1.0,\n",
       " 0.99998630794254284,\n",
       " 0.99999706473338779,\n",
       " 0.99997266959688635,\n",
       " 0.99998849671852796,\n",
       " 0.99999386917650634,\n",
       " 0.99999896786293074,\n",
       " 0.99997901628095065,\n",
       " 0.99999147851288328,\n",
       " 0.99999263644544933,\n",
       " 1.0,\n",
       " 0.99999671197684914,\n",
       " 0.99999580118440345,\n",
       " 0.99999805525936558,\n",
       " 0.99998776182182703,\n",
       " 0.99999999999999989,\n",
       " 0.99993402964461764,\n",
       " 0.99999823246839092,\n",
       " 0.99996231987804907,\n",
       " 0.99998310332335139,\n",
       " 0.99998123691972629,\n",
       " 0.99994789342249646,\n",
       " 0.99995291345941673,\n",
       " 0.99996280085860678,\n",
       " 0.99999682372185184,\n",
       " 0.99953379569503098,\n",
       " 0.99999670505056726,\n",
       " 0.99999658801739366,\n",
       " 0.99996524133908071,\n",
       " 0.9999943112676476,\n",
       " 0.99998233964093097,\n",
       " 0.99999547549225865,\n",
       " 0.99998876715779172,\n",
       " 0.99999232679752714,\n",
       " 0.99994454855555259,\n",
       " 0.99999969265958211,\n",
       " 0.99994527302184433,\n",
       " 0.99999644790484155,\n",
       " 0.99999410859034954,\n",
       " 0.99996998003771187,\n",
       " 0.99998062984876768,\n",
       " 0.99995686999860656,\n",
       " 0.99997114716342661,\n",
       " 0.99999352756427695,\n",
       " 0.99999999999999978,\n",
       " 0.99995740253256593,\n",
       " 0.99999695149529666,\n",
       " 0.99998820856648907,\n",
       " 0.99996125049900231,\n",
       " 0.99999862952742313,\n",
       " 0.99999259036495958,\n",
       " 0.99999165718950933,\n",
       " 0.99997108081920516,\n",
       " 0.99998856757715138,\n",
       " 0.9999782473168507,\n",
       " 0.99997125529905861,\n",
       " 0.99999629761579512,\n",
       " 0.99999178835092872,\n",
       " 0.99997597265878813,\n",
       " 0.99999915616526269,\n",
       " 0.9999978059201704,\n",
       " 0.99997948924376268,\n",
       " 0.99999641194583588,\n",
       " 0.99999520992355184,\n",
       " 0.99996874818586279,\n",
       " 0.99988934992138678,\n",
       " 0.9999942020241992,\n",
       " 0.99999827868627511,\n",
       " 0.99999720471622544,\n",
       " 0.99998077155225662,\n",
       " 0.99999435424144567,\n",
       " 0.99998387479122042,\n",
       " 0.99999462767471203,\n",
       " 0.99998443995377884,\n",
       " 0.99997213275626573,\n",
       " 0.99999537714422659,\n",
       " 0.9999945606891506,\n",
       " 0.99998522029204984,\n",
       " 0.99993812880455613,\n",
       " 0.99999047266779739,\n",
       " 0.99999370836382451,\n",
       " 0.99999416306872446,\n",
       " 0.99999339143496058,\n",
       " 0.99999426276747905,\n",
       " 0.99998377562154117,\n",
       " 0.99999453897267032,\n",
       " 0.99997709269110779,\n",
       " 0.99993508171362067,\n",
       " 0.99995733223254246,\n",
       " 0.99997326977479395,\n",
       " 0.99994513263582685,\n",
       " 0.99995352954852057,\n",
       " 0.99999571167050827,\n",
       " 0.99999999999999989,\n",
       " 0.99999402092276257,\n",
       " 0,\n",
       " 0.99999364943419866,\n",
       " 0.99994829786061601,\n",
       " 0.99989463649070931,\n",
       " 1.0000000000000002,\n",
       " 0.9999898206287593,\n",
       " 0.99998810727332976,\n",
       " 0.99996297382327826,\n",
       " 0.99994732193767899,\n",
       " 0.99999656881258003,\n",
       " 0.99993375112408611,\n",
       " 0.99999732555022136,\n",
       " 0.99999442367000135,\n",
       " 0.99999156828664459,\n",
       " 0.99998932890424019,\n",
       " 0.99997587536156907,\n",
       " 0.99998987830720254,\n",
       " 0.99998860722217242,\n",
       " 0.99997593305435506,\n",
       " 0.99996800729646318,\n",
       " 0.99999623820802552,\n",
       " 0.99999999999999967,\n",
       " 0.99999999999999911,\n",
       " 0.99997281138707139,\n",
       " 0.99998960665095016,\n",
       " 0.99999173801333252,\n",
       " 0.9999966691046176,\n",
       " 0.99999050261639733,\n",
       " 0.99997942747392587,\n",
       " 0.99997775464598659,\n",
       " 0.99998814836931027,\n",
       " 0.99999840557995612,\n",
       " 0.99989952976023011,\n",
       " 0.99999326465516214,\n",
       " 0.99998565532924344,\n",
       " 0.99999225985780082,\n",
       " 0.99999602465419901,\n",
       " 0.99998615387656153,\n",
       " 1.0000000000000002,\n",
       " 0.99998470660494143,\n",
       " 0.99999920835265321,\n",
       " 0.99999048595897566,\n",
       " 0.99997316032204975,\n",
       " 0.99998552992908507,\n",
       " 0.99998967782102044,\n",
       " 1.0,\n",
       " 0.99993749088832018,\n",
       " 0.9999790730963678,\n",
       " 0.99999292600333278,\n",
       " 0.99999674505239933,\n",
       " 0.9999958980661291,\n",
       " 0.99996604883223539,\n",
       " 0.99998660290024399,\n",
       " 0.99999801378974895,\n",
       " 0.99995799870808222,\n",
       " 0.9999973301923768,\n",
       " 0.99999587072605067,\n",
       " 0.99999644868724236,\n",
       " 0.99999971689497913,\n",
       " 0.99998421767702861,\n",
       " 0.99999493177363785,\n",
       " 0.99998962507869682,\n",
       " 0.99999521818802184,\n",
       " 0.99992581758487253,\n",
       " 0.9999984785421856,\n",
       " 0.99999682800853573,\n",
       " 0.99996682991381713,\n",
       " 0.99999999999999922,\n",
       " 0.99997157889867483,\n",
       " 0.99998938832521256,\n",
       " 0.99999025762129801,\n",
       " 0.99999999999999978,\n",
       " 0.99998749952860766,\n",
       " 0.99998980026399364,\n",
       " 0.99999685643076119,\n",
       " 0.99996868523089866,\n",
       " 0.99998396299230496,\n",
       " 0.99996905478756015,\n",
       " 0.99997989971420642,\n",
       " 0.99995821996584944,\n",
       " 0.99999701888681825,\n",
       " 0.9999305899531864]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
