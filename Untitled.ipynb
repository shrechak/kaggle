{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/indix/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 18:50:14,726 : INFO : collecting all words and their counts\n",
      "2017-05-16 18:50:14,728 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-16 18:50:15,506 : INFO : PROGRESS: at sentence #10000, processed 219770 words, keeping 23488 word types\n",
      "2017-05-16 18:50:16,233 : INFO : PROGRESS: at sentence #20000, processed 430477 words, keeping 34367 word types\n",
      "2017-05-16 18:50:17,022 : INFO : PROGRESS: at sentence #30000, processed 669056 words, keeping 42365 word types\n",
      "2017-05-16 18:50:17,769 : INFO : PROGRESS: at sentence #40000, processed 888291 words, keeping 49136 word types\n",
      "2017-05-16 18:50:18,333 : INFO : PROGRESS: at sentence #50000, processed 1039920 words, keeping 53024 word types\n",
      "2017-05-16 18:50:18,795 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 57340 sentences\n",
      "2017-05-16 18:50:18,796 : INFO : Loading a fresh vocabulary\n",
      "2017-05-16 18:50:19,035 : INFO : min_count=1 retains 56057 unique words (100% of original 56057, drops 0)\n",
      "2017-05-16 18:50:19,036 : INFO : min_count=1 leaves 1161192 word corpus (100% of original 1161192, drops 0)\n",
      "2017-05-16 18:50:19,295 : INFO : deleting the raw counts dictionary of 56057 items\n",
      "2017-05-16 18:50:19,296 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-05-16 18:50:19,297 : INFO : downsampling leaves estimated 854152 word corpus (73.6% of prior 1161192)\n",
      "2017-05-16 18:50:19,298 : INFO : estimated required memory for 56057 words and 100 dimensions: 72874100 bytes\n",
      "2017-05-16 18:50:19,502 : INFO : resetting layer weights\n",
      "2017-05-16 18:50:20,526 : INFO : training model with 3 workers on 56057 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-16 18:50:20,527 : INFO : expecting 57340 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-16 18:50:21,531 : INFO : PROGRESS: at 3.64% examples, 169698 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:22,547 : INFO : PROGRESS: at 8.00% examples, 182331 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:23,576 : INFO : PROGRESS: at 11.81% examples, 183269 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:24,591 : INFO : PROGRESS: at 16.44% examples, 180792 words/s, in_qsize 1, out_qsize 0\n",
      "2017-05-16 18:50:25,615 : INFO : PROGRESS: at 21.09% examples, 177720 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:26,639 : INFO : PROGRESS: at 25.35% examples, 179388 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:27,645 : INFO : PROGRESS: at 29.37% examples, 181609 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:28,664 : INFO : PROGRESS: at 33.68% examples, 184127 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:29,684 : INFO : PROGRESS: at 39.08% examples, 182917 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:30,712 : INFO : PROGRESS: at 42.99% examples, 181146 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:31,714 : INFO : PROGRESS: at 47.18% examples, 182060 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:32,734 : INFO : PROGRESS: at 51.30% examples, 183580 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:33,777 : INFO : PROGRESS: at 56.31% examples, 184071 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:34,799 : INFO : PROGRESS: at 61.17% examples, 183297 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:35,822 : INFO : PROGRESS: at 65.42% examples, 183631 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:36,841 : INFO : PROGRESS: at 69.32% examples, 183741 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:37,862 : INFO : PROGRESS: at 73.14% examples, 183939 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:38,889 : INFO : PROGRESS: at 78.10% examples, 182437 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:39,914 : INFO : PROGRESS: at 82.44% examples, 181940 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:40,925 : INFO : PROGRESS: at 86.66% examples, 182334 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:41,944 : INFO : PROGRESS: at 90.75% examples, 183175 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:42,963 : INFO : PROGRESS: at 95.46% examples, 183677 words/s, in_qsize 0, out_qsize 0\n",
      "2017-05-16 18:50:43,790 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 18:50:43,804 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 18:50:43,809 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 18:50:43,810 : INFO : training on 5805960 raw words (4270459 effective words) took 23.3s, 183430 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = brown.sents()\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 18:50:08,226 : INFO : collecting all words and their counts\n",
      "2017-05-16 18:50:08,227 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-05-16 18:50:08,334 : INFO : PROGRESS: at example #10000, processed 219770 words (2062821/s), 23488 word types, 10000 tags\n",
      "2017-05-16 18:50:08,431 : INFO : PROGRESS: at example #20000, processed 430477 words (2201958/s), 34367 word types, 20000 tags\n",
      "2017-05-16 18:50:08,534 : INFO : PROGRESS: at example #30000, processed 669056 words (2337382/s), 42365 word types, 30000 tags\n",
      "2017-05-16 18:50:08,633 : INFO : PROGRESS: at example #40000, processed 888291 words (2242577/s), 49136 word types, 40000 tags\n",
      "2017-05-16 18:50:08,829 : INFO : PROGRESS: at example #50000, processed 1039920 words (776680/s), 53024 word types, 50000 tags\n",
      "2017-05-16 18:50:08,894 : INFO : collected 56057 word types and 57340 unique tags from a corpus of 57340 examples and 1161192 words\n",
      "2017-05-16 18:50:08,894 : INFO : Loading a fresh vocabulary\n",
      "2017-05-16 18:50:09,047 : INFO : min_count=1 retains 56057 unique words (100% of original 56057, drops 0)\n",
      "2017-05-16 18:50:09,048 : INFO : min_count=1 leaves 1161192 word corpus (100% of original 1161192, drops 0)\n",
      "2017-05-16 18:50:09,315 : INFO : deleting the raw counts dictionary of 56057 items\n",
      "2017-05-16 18:50:09,317 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-05-16 18:50:09,320 : INFO : downsampling leaves estimated 854152 word corpus (73.6% of prior 1161192)\n",
      "2017-05-16 18:50:09,321 : INFO : estimated required memory for 56057 words and 100 dimensions: 107278100 bytes\n",
      "2017-05-16 18:50:09,547 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "i=0\n",
    "for sent in brown.sents():\n",
    "    sentences.append(models.doc2vec.LabeledSentence(words=sent,tags=[\"SENT_\"+str(i)]))\n",
    "    i = i+1\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    def __iter__(self):\n",
    "        for uid, line in enumerate(open(filename)):\n",
    "            yield LabeledSentence(words=line.split(), labels=['SENT_%s' % uid])\n",
    "            \n",
    "model2 = models.Doc2Vec(alpha=.025, min_alpha=.025, min_count=1)\n",
    "model2.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import*\n",
    "from decimal import Decimal\n",
    "\n",
    "def euclidean_distance(x,y):\n",
    "    return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
    "\n",
    "def manhattan_distance(x,y):\n",
    "    return sum(abs(a-b) for a,b in zip(x,y))\n",
    "\n",
    "\n",
    "def nth_root(value, n_root):\n",
    "    root_value = 1/float(n_root)\n",
    "    return round (Decimal(value) ** Decimal(root_value),3)\n",
    "\n",
    "def minkowski_distance(x,y,p_value):\n",
    "    return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),\n",
    "       p_value)\n",
    "\n",
    "def jaccard_similarity(x,y):\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'], dtype='object')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')[:100]\n",
    "df = df[0:8]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "euc_dist = []\n",
    "man_dist = []\n",
    "mink_dist = []\n",
    "cos_sim = []\n",
    "jac_sim = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sent1 = row.question1\n",
    "    sent2 = row.question2\n",
    "    words1 = list(filter(lambda x: x.isalnum() and x not in stop, nltk.word_tokenize(sent1.lower())))\n",
    "    words2 = list(filter(lambda x: x.isalnum() and x not in stop, nltk.word_tokenize(sent2.lower())))\n",
    "    cos_sim.append(model2.docvecs.similarity_unseen_docs(model2, words1, words2))\n",
    "    v1 = model2.infer_vector(words1)\n",
    "    v2 = model2.infer_vector(words2)\n",
    "    euc_dist.append(euclidean_distance(v1, v2))\n",
    "    man_dist.append(manhattan_distance(v1, v2))\n",
    "    mink_dist.append(minkowski_distance(v1, v2, 2))\n",
    "    jac = len(set(words1).intersection(words2))/len(set(words1).union(words2))\n",
    "    jac_sim.append(jac)\n",
    "\n",
    "df['euc_dist'] = euc_dist\n",
    "df['man_dist'] = man_dist\n",
    "df['mink_dist'] = mink_dist\n",
    "df['cos_sim'] = cos_sim\n",
    "df['jac_sim'] = jac_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>euc_dist</th>\n",
       "      <th>man_dist</th>\n",
       "      <th>mink_dist</th>\n",
       "      <th>cos_sim</th>\n",
       "      <th>jac_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035004</td>\n",
       "      <td>0.286313</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.189670</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037735</td>\n",
       "      <td>0.312456</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.029138</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039158</td>\n",
       "      <td>0.314048</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.113387</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048946</td>\n",
       "      <td>0.417823</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.382871</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041947</td>\n",
       "      <td>0.346023</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.007809</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.038444</td>\n",
       "      <td>0.317944</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.076028</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040654</td>\n",
       "      <td>0.334385</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.024858</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040726</td>\n",
       "      <td>0.337266</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.023461</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "5   5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6   6    13    14                                Should I buy tiago?   \n",
       "7   7    15    16                     How can I be a good geologist?   \n",
       "\n",
       "                                           question2  is_duplicate  euc_dist  \\\n",
       "0  What is the step by step guide to invest in sh...             0  0.035004   \n",
       "1  What would happen if the Indian government sto...             0  0.037735   \n",
       "2  How can Internet speed be increased by hacking...             0  0.039158   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  0.048946   \n",
       "4            Which fish would survive in salt water?             0  0.041947   \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...             1  0.038444   \n",
       "6  What keeps childern active and far from phone ...             0  0.040654   \n",
       "7          What should I do to be a great geologist?             1  0.040726   \n",
       "\n",
       "   man_dist mink_dist   cos_sim   jac_sim  \n",
       "0  0.286313     0.035  0.189670  0.833333  \n",
       "1  0.312456     0.038  0.029138  0.222222  \n",
       "2  0.314048     0.039  0.113387  0.222222  \n",
       "3  0.417823     0.049 -0.382871  0.000000  \n",
       "4  0.346023     0.042  0.007809  0.153846  \n",
       "5  0.317944     0.038  0.076028  0.444444  \n",
       "6  0.334385     0.041  0.024858  0.000000  \n",
       "7  0.337266     0.041  0.023461  0.333333  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 19:07:37,236 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('experience', 0.9319432973861694),\n",
       " ('limited', 0.9238691329956055),\n",
       " ('common', 0.9205867052078247),\n",
       " ('almost', 0.9118727445602417),\n",
       " ('certain', 0.9115947484970093),\n",
       " ('new', 0.9058680534362793),\n",
       " ('interest', 0.9034856557846069),\n",
       " ('individual', 0.9001429080963135),\n",
       " ('indication', 0.8998943567276001),\n",
       " ('part', 0.898358166217804)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.012604972658712167"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model2.infer_vector(words1[0])\n",
    "v2 = model2.infer_vector(words2[0])\n",
    "\n",
    "model2.similarity(words1[0], words2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04167559272721855"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distance(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
